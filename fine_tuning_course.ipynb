{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6481c9",
   "metadata": {},
   "source": [
    "# Fine-Tuning Machine Learning Models: A Comprehensive Course\n",
    "\n",
    "Fine-tuning is a powerful technique that allows you to adapt pre-trained models to specific tasks or domains. This notebook provides a practical guide to fine-tuning various types of machine learning models.\n",
    "\n",
    "## What is Fine-Tuning?\n",
    "\n",
    "Fine-tuning is the process of taking a model that has been pre-trained on a large dataset and then further training it on a smaller, task-specific dataset. This approach leverages the knowledge already captured in the pre-trained model and adapts it to perform well on a new, related task.\n",
    "\n",
    "## Why Fine-Tune?\n",
    "\n",
    "- **Resource efficiency**: Training from scratch requires significant computational resources and data\n",
    "- **Better performance**: Pre-trained models contain valuable features and patterns\n",
    "- **Faster convergence**: Fine-tuning typically requires fewer iterations than training from scratch\n",
    "- **Less data needed**: You can achieve good results with smaller domain-specific datasets\n",
    "\n",
    "## Course Outline\n",
    "\n",
    "1. Understanding Transfer Learning & Fine-Tuning\n",
    "2. Preparing Data for Fine-Tuning\n",
    "3. Fine-Tuning Computer Vision Models\n",
    "4. Fine-Tuning NLP Models\n",
    "5. Fine-Tuning Large Language Models (LLMs)\n",
    "6. Evaluation and Hyperparameter Tuning\n",
    "7. Best Practices and Common Pitfalls\n",
    "8. Advanced Fine-Tuning Techniques\n",
    "\n",
    "## Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3238ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q torch torchvision transformers datasets evaluate sklearn matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510875c9",
   "metadata": {},
   "source": [
    "## 1. Understanding Transfer Learning & Fine-Tuning\n",
    "\n",
    "Transfer learning is a machine learning technique where knowledge gained from solving one problem is applied to a different but related problem. Fine-tuning is a specific approach to transfer learning.\n",
    "\n",
    "### Types of Transfer Learning:\n",
    "\n",
    "1. **Feature Extraction**: The pre-trained model is used only as a feature extractor. The last layer(s) are removed and replaced with a new classifier that is trained on the new task.\n",
    "\n",
    "2. **Fine-Tuning**: Some or all of the weights in the pre-trained model are updated during training on the new task.\n",
    "\n",
    "### Common Approaches to Fine-Tuning:\n",
    "\n",
    "- **Freeze early layers, train later layers**: The early layers of neural networks typically learn generic features, while later layers learn more task-specific features.\n",
    "- **Gradual unfreezing**: Start by training only the last layer, then progressively unfreeze and train earlier layers.\n",
    "- **Differential learning rates**: Apply smaller learning rates to early layers and larger learning rates to later layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ddff8",
   "metadata": {},
   "source": [
    "## 2. Preparing Data for Fine-Tuning\n",
    "\n",
    "Data preparation is crucial for effective fine-tuning. Let's walk through the process:\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Data Collection**: Gather task-specific data relevant to your application\n",
    "2. **Data Cleaning**: Remove inconsistencies, handle missing values\n",
    "3. **Data Preprocessing**: Format data to match the pre-trained model's requirements\n",
    "4. **Data Augmentation**: Increase dataset diversity artificially \n",
    "5. **Dataset Splitting**: Create training, validation, and test sets\n",
    "\n",
    "Let's demonstrate these steps with a practical example using a sample image classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transformations for data preprocessing and augmentation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),  # Data augmentation\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),  # Data augmentation\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load a sample dataset (CIFAR-10 in this example)\n",
    "# In real applications, you would use your own domain-specific dataset\n",
    "cifar_data = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                          download=True)\n",
    "\n",
    "# Create smaller dataset for fine-tuning example\n",
    "# Let's assume we're only interested in the first 2 classes\n",
    "indices = np.where(np.array(cifar_data.targets) < 2)[0]\n",
    "subset_data = torch.utils.data.Subset(cifar_data, indices)\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(subset_data))\n",
    "val_size = len(subset_data) - train_size\n",
    "train_subset, val_subset = random_split(subset_data, [train_size, val_size])\n",
    "\n",
    "# Apply transformations\n",
    "class TransformedSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.subset[idx]\n",
    "        return self.transform(x), y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "train_data = TransformedSubset(train_subset, data_transforms['train'])\n",
    "val_data = TransformedSubset(val_subset, data_transforms['val'])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Visualize a few examples\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images[:5]))\n",
    "class_names = ['airplane', 'automobile']\n",
    "print('Sample classes: ', [class_names[labels[i]] for i in range(5)])\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e3a3a",
   "metadata": {},
   "source": [
    "## 3. Fine-Tuning Computer Vision Models\n",
    "\n",
    "Computer vision models are commonly fine-tuned for tasks like:\n",
    "- Object detection\n",
    "- Image classification\n",
    "- Semantic segmentation\n",
    "- Instance segmentation\n",
    "\n",
    "Let's fine-tune a pre-trained ResNet model for our specific image classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafbd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "from torchvision import models\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Inspect the model architecture\n",
    "print(\"Original model's last layer:\", model.fc)\n",
    "\n",
    "# Modify the final fully connected layer for our specific task (2 classes)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # Replace with number of target classes\n",
    "\n",
    "print(\"Modified model's last layer:\", model.fc)\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tuning approach 1: Train only the final layer\n",
    "# Freeze all layers except the final one\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Unfreeze the final layer\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Use a smaller learning rate for fine-tuning\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, dataloaders, num_epochs=5):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    train_history = {'loss': [], 'acc': []}\n",
    "    val_history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                dataloader = val_loader\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                # Track history only in train phase\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward + optimize only in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # Save history\n",
    "            if phase == 'train':\n",
    "                train_history['loss'].append(epoch_loss)\n",
    "                train_history['acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                val_history['loss'].append(epoch_loss)\n",
    "                val_history['acc'].append(epoch_acc.item())\n",
    "            \n",
    "            # Deep copy the model if best performance\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, train_history, val_history\n",
    "\n",
    "# Train the model - feature extraction approach\n",
    "model_ft, train_history, val_history = train_model(model, criterion, optimizer, \n",
    "                                                  [train_loader, val_loader], num_epochs=5)\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_history['loss'], label='train')\n",
    "plt.plot(val_history['loss'], label='val')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_history['acc'], label='train')\n",
    "plt.plot(val_history['acc'], label='val')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0c7da",
   "metadata": {},
   "source": [
    "### Fine-Tuning the Entire Network\n",
    "\n",
    "Now, let's try a different fine-tuning approach where we update all layers of the network but use different learning rates for different parts of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e205e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Fine-tuning approach 2: Fine-tune the entire network with differential learning rates\n",
    "# Parameters of newly constructed layers have different learning rates\n",
    "params_to_update = [\n",
    "    {'params': [param for name, param in model.named_parameters() if 'fc' not in name], 'lr': 0.0001},\n",
    "    {'params': model.fc.parameters(), 'lr': 0.001}\n",
    "]\n",
    "\n",
    "optimizer = optim.Adam(params_to_update)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model - full fine-tuning approach\n",
    "model_ft_full, train_history_full, val_history_full = train_model(model, criterion, optimizer, \n",
    "                                                                [train_loader, val_loader], num_epochs=5)\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_history_full['loss'], label='train')\n",
    "plt.plot(val_history_full['loss'], label='val')\n",
    "plt.title('Loss (Full Fine-tuning)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_history_full['acc'], label='train')\n",
    "plt.plot(val_history_full['acc'], label='val')\n",
    "plt.title('Accuracy (Full Fine-tuning)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compare approaches\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_history['acc'], label='feature extraction')\n",
    "plt.plot(train_history_full['acc'], label='full fine-tuning')\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_history['acc'], label='feature extraction')\n",
    "plt.plot(val_history_full['acc'], label='full fine-tuning')\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5aa271",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning NLP Models\n",
    "\n",
    "Natural Language Processing (NLP) models can be fine-tuned for tasks like:\n",
    "- Text classification\n",
    "- Named Entity Recognition\n",
    "- Question answering\n",
    "- Sentiment analysis\n",
    "\n",
    "Let's fine-tune a pre-trained BERT model for a text classification task using the Hugging Face `transformers` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f39ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load a dataset - we'll use the IMDB movie reviews for sentiment analysis\n",
    "imdb = load_dataset(\"imdb\")\n",
    "print(imdb)\n",
    "\n",
    "# Take a smaller subset for demonstration purposes\n",
    "train_dataset = imdb[\"train\"].shuffle(seed=42).select(range(5000))\n",
    "test_dataset = imdb[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Load tokenizer and model from Hugging Face\n",
    "model_name = \"distilbert-base-uncased\"  # Smaller, faster version of BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Define metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/imdb_classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Test on some examples\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a text classification pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test some examples\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! The acting was superb and the storyline captivating.\",\n",
    "    \"I really didn't enjoy this film. The plot was confusing and the characters poorly developed.\"\n",
    "]\n",
    "\n",
    "for review in test_reviews:\n",
    "    result = classifier(review)\n",
    "    sentiment = \"positive\" if result[0]['label'] == \"LABEL_1\" else \"negative\"\n",
    "    confidence = result[0]['score']\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Sentiment: {sentiment}, Confidence: {confidence:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea1a1f",
   "metadata": {},
   "source": [
    "## 5. Fine-Tuning Large Language Models (LLMs)\n",
    "\n",
    "Large Language Models (LLMs) like GPT, BERT, and T5 can be fine-tuned for specific domains and tasks. Let's explore instruction fine-tuning with a smaller model:\n",
    "\n",
    "### Use Cases for LLM Fine-Tuning:\n",
    "\n",
    "- **Domain adaptation**: Tailor the model to specific industries like legal, medical, or financial\n",
    "- **Task-specific tuning**: Optimize for tasks like summarization or question-answering\n",
    "- **Instruction tuning**: Teach the model to follow specific instructions\n",
    "- **Alignment**: Ensure outputs match human preferences and values\n",
    "\n",
    "Let's fine-tune a small T5 model for a simple summarization task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers.data.data_collator import DataCollatorForSeq2Seq\n",
    "\n",
    "# Load a dataset - CNN/DailyMail for summarization\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "print(dataset)\n",
    "\n",
    "# Take a small subset for demonstration\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(500))\n",
    "val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"t5-small\"  # Using a smaller model for demonstration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing function\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"highlights\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Load evaluation metric\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(pred.split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.split()) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract the median scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return result\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results/t5-summarization\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Test the model on a few examples\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "articles = [\n",
    "    \"\"\"The US has passed the peak of new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month. \n",
    "    The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world. \n",
    "    At the daily White House coronavirus briefing on Wednesday, Trump said new guidelines to reopen the country would be announced on Thursday after he speaks to governors. \n",
    "    \"We'll be the comeback kids, all of us,\" he said. \"We want to get our country back.\" The Trump administration has previously fixed May 1 as a possible date to reopen the world's largest economy, but the president said some states may be able to return to normalcy sooner than others.\"\"\"\n",
    "]\n",
    "\n",
    "for article in articles:\n",
    "    summary = summarizer(prefix + article, max_length=100, min_length=30, do_sample=False)\n",
    "    print(f\"Article: {article[:100]}...\\n\")\n",
    "    print(f\"Generated summary: {summary[0]['summary_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26753e",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Hyperparameter Tuning\n",
    "\n",
    "Properly evaluating your fine-tuned models and optimizing their hyperparameters is crucial for achieving the best performance.\n",
    "\n",
    "### Key Evaluation Metrics:\n",
    "\n",
    "- **Classification**: Accuracy, F1-score, AUC-ROC, confusion matrix\n",
    "- **NLP**: BLEU, ROUGE, METEOR, perplexity\n",
    "- **Computer Vision**: mAP, IoU\n",
    "\n",
    "Let's implement a simple hyperparameter tuning process using scikit-learn's GridSearchCV with our CV model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a wrapper class for scikit-learn compatibility\n",
    "class ModelWrapper:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X = torch.tensor(X).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        return preds.cpu().numpy()\n",
    "    \n",
    "    def fit(self, X, y, learning_rate=0.001, epochs=5):\n",
    "        # Convert data to tensors\n",
    "        X = torch.tensor(X).to(self.device)\n",
    "        y = torch.tensor(y).to(self.device)\n",
    "        \n",
    "        # Set up optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Train\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            outputs = self.model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        return self\n",
    "\n",
    "# For simplicity in this example, we'll use a very small subset and a simple model\n",
    "# In practice, you would do this with your full dataset and your fine-tuned model\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a simple model for demonstration\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)  # Simplified for MNIST\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load a simple dataset (MNIST) for demonstration\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Take a subset with only digits 0 and 1 for binary classification\n",
    "idx_train = (mnist_train.targets == 0) | (mnist_train.targets == 1)\n",
    "idx_test = (mnist_test.targets == 0) | (mnist_test.targets == 1)\n",
    "\n",
    "X_train = mnist_train.data[idx_train].numpy()\n",
    "y_train = mnist_train.targets[idx_train].numpy()\n",
    "X_test = mnist_test.data[idx_test].numpy()\n",
    "y_test = mnist_test.targets[idx_test].numpy()\n",
    "\n",
    "# Take small subsets for demonstration\n",
    "X_train, y_train = X_train[:500], y_train[:500]\n",
    "X_test, y_test = X_test[:100], y_test[:100]\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Create model\n",
    "model = SimpleModel().to(device)\n",
    "wrapper = ModelWrapper(model, device)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'epochs': [3, 5, 10]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=wrapper, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Display classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Visualize some predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {y_test[i]}, Pred: {y_pred[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bd388",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Common Pitfalls\n",
    "\n",
    "When fine-tuning models, several best practices can help you achieve better results:\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start simple**: Begin with a frozen pre-trained model and only train the final layer.\n",
    "\n",
    "2. **Gradual unfreezing**: If needed, gradually unfreeze and train earlier layers.\n",
    "\n",
    "3. **Lower learning rates**: Use a smaller learning rate than you would for training from scratch.\n",
    "\n",
    "4. **Differential learning rates**: Use even smaller learning rates for pre-trained layers.\n",
    "\n",
    "5. **Early stopping**: Monitor validation performance to prevent overfitting.\n",
    "\n",
    "6. **Regularization**: Use techniques like weight decay, dropout, or data augmentation.\n",
    "\n",
    "7. **Batch normalization**: Re-calibrate batch normalization statistics for your dataset.\n",
    "\n",
    "8. **Monitor training**: Keep track of both training and validation metrics.\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Catastrophic forgetting**: The model forgets previously learned information.\n",
    "\n",
    "2. **Overfitting**: The model performs well on training data but poorly on new data.\n",
    "\n",
    "3. **Inappropriate learning rate**: Too high can destabilize training; too low can lead to slow convergence.\n",
    "\n",
    "4. **Not adapting preprocessing**: Ensure your data preprocessing matches what the pre-trained model expects.\n",
    "\n",
    "5. **Ignoring class imbalance**: Address imbalanced classes in your dataset.\n",
    "\n",
    "6. **Training-validation mismatch**: Ensure validation process matches how you'll use the model.\n",
    "\n",
    "7. **Neglecting model size**: Consider computational constraints, especially for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecbb8cb",
   "metadata": {},
   "source": [
    "## 8. Advanced Fine-Tuning Techniques\n",
    "\n",
    "As you progress, you might want to explore more advanced fine-tuning techniques:\n",
    "\n",
    "### Parameter-Efficient Fine-Tuning\n",
    "\n",
    "1. **Adapters**: Add small trainable modules between layers while keeping pre-trained weights frozen.\n",
    "\n",
    "2. **LoRA (Low-Rank Adaptation)**: Represent weight updates as low-rank decompositions.\n",
    "\n",
    "3. **Prompt Tuning**: Learn soft prompts while keeping the model frozen.\n",
    "\n",
    "4. **QLoRA**: Quantized Low-Rank Adaptation for efficient fine-tuning.\n",
    "\n",
    "Let's implement a simple adapter-based approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple adapter implementation\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, input_dim, adapter_dim):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(input_dim, adapter_dim)\n",
    "        self.up = nn.Linear(adapter_dim, input_dim)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Initialize to small values\n",
    "        nn.init.normal_(self.down.weight, std=1e-3)\n",
    "        nn.init.normal_(self.up.weight, std=1e-3)\n",
    "        nn.init.zeros_(self.down.bias)\n",
    "        nn.init.zeros_(self.up.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.up(F.relu(self.down(x))) + x  # Residual connection\n",
    "\n",
    "# Modify an existing ResNet model to add adapters\n",
    "def add_adapters_to_resnet(model, adapter_dim=64):\n",
    "    # Add adapters to the ResNet blocks\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, models.resnet.BasicBlock) or isinstance(module, models.resnet.Bottleneck):\n",
    "            # Get the dimension of the output\n",
    "            if isinstance(module, models.resnet.BasicBlock):\n",
    "                input_dim = module.conv2.out_channels\n",
    "            else:  # Bottleneck\n",
    "                input_dim = module.conv3.out_channels\n",
    "                \n",
    "            # Create and add adapter\n",
    "            module.adapter = Adapter(input_dim, adapter_dim)\n",
    "            \n",
    "            # Save original forward method\n",
    "            original_forward = module.forward\n",
    "            \n",
    "            # Define new forward method with adapter\n",
    "            def new_forward(self, x):\n",
    "                identity = self.downsample(x) if self.downsample is not None else x\n",
    "                \n",
    "                out = self.conv1(x)\n",
    "                out = self.bn1(out)\n",
    "                out = self.relu(out)\n",
    "                \n",
    "                out = self.conv2(out)\n",
    "                out = self.bn2(out)\n",
    "                \n",
    "                if hasattr(self, 'conv3'):  # For Bottleneck\n",
    "                    out = self.relu(out)\n",
    "                    out = self.conv3(out)\n",
    "                    out = self.bn3(out)\n",
    "                \n",
    "                out += identity\n",
    "                out = self.relu(out)\n",
    "                \n",
    "                # Apply adapter\n",
    "                out = self.adapter(out)\n",
    "                \n",
    "                return out\n",
    "                \n",
    "            # Bind the new method to the module\n",
    "            import types\n",
    "            module.forward = types.MethodType(new_forward, module)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a model with adapters\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add adapters\n",
    "model = add_adapters_to_resnet(model, adapter_dim=16)\n",
    "\n",
    "# Unfreeze only the adapters and the final layer\n",
    "for name, param in model.named_parameters():\n",
    "    if 'adapter' in name or 'fc' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Count trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Percentage of trainable parameters: {trainable_params/total_params*100:.2f}%\")\n",
    "\n",
    "# Train with adapters (would use the same training loop as before)\n",
    "# For brevity, we won't repeat the full training code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d4b54",
   "metadata": {},
   "source": [
    "## Implementation Strategies for LLM Fine-Tuning\n",
    "\n",
    "For very large language models, specialized techniques are often needed:\n",
    "\n",
    "### Full Fine-Tuning vs. Parameter-Efficient Methods\n",
    "\n",
    "| Technique | Description | Pros | Cons |\n",
    "|-----------|-------------|------|------|\n",
    "| **Full Fine-Tuning** | Update all model weights | Best performance | High compute requirements |\n",
    "| **LoRA** | Low-rank adaptation of weight matrices | Much less memory | Slightly lower performance |\n",
    "| **QLoRA** | Quantized LoRA | Even more efficient | Complex implementation |\n",
    "| **Prefix/Prompt Tuning** | Add trainable tokens | Very parameter efficient | Task-specific |\n",
    "| **Adapters** | Small trainable layers | Modular, stackable | Added inference latency |\n",
    "\n",
    "### Knowledge Distillation\n",
    "\n",
    "Another technique is to fine-tune a smaller model to mimic a larger one:\n",
    "\n",
    "1. Fine-tune a large model on your task\n",
    "2. Use that model to generate outputs on your dataset\n",
    "3. Train a smaller model to match those outputs\n",
    "\n",
    "This allows you to leverage the capabilities of large models while deploying more efficient ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49716a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Fine-tuning pre-trained models is a powerful technique that lets you leverage the knowledge embedded in large models while adapting them to your specific needs. The key takeaways from this course include:\n",
    "\n",
    "1. **Transfer learning efficiency**: Fine-tuning is more efficient than training from scratch in terms of data, compute, and time requirements.\n",
    "\n",
    "2. **Approach options**: Choose between feature extraction, full fine-tuning, or parameter-efficient methods based on your resources and needs.\n",
    "\n",
    "3. **Hyperparameter sensitivity**: Fine-tuning requires careful tuning of learning rates and other hyperparameters.\n",
    "\n",
    "4. **Evaluation importance**: Proper evaluation on relevant metrics ensures your model generalizes well.\n",
    "\n",
    "5. **Advanced techniques**: As models grow larger, parameter-efficient methods become increasingly important.\n",
    "\n",
    "Whether you're working with computer vision, NLP, or other domains, the principles of fine-tuning remain similar: leverage pre-trained knowledge while carefully adapting the model to your specific task.\n",
    "\n",
    "## Further Resources\n",
    "\n",
    "- [Hugging Face Transfer Learning Documentation](https://huggingface.co/docs/transformers/training)\n",
    "- [PyTorch Transfer Learning Tutorials](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "- [Parameter-Efficient Fine-Tuning Methods](https://arxiv.org/abs/2303.15647)\n",
    "- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
