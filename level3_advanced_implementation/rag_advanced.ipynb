{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29efdd07",
   "metadata": {},
   "source": [
    "# Level 3: Advanced RAG Implementation (Python)\n",
    "\n",
    "This notebook demonstrates an advanced, modular, and best-practices implementation of a Retrieval-Augmented Generation (RAG) system for PDF documents. It uses object-oriented design, clear separation of concerns, and includes comments and documentation for advanced learners. It integrates with real services including OpenAI, ChromaDB, and Hugging Face transformers.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "- **Class-based design** for extensibility and maintainability\n",
    "- **Dependency injection** for easy testing and swapping components\n",
    "- **Docstrings and type hints** for clarity\n",
    "- **Reusable utility functions**\n",
    "- **Real-world integrations** with OpenAI API, ChromaDB, and Hugging Face\n",
    "- **Testable with example cases**\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "Below, we define the main components as classes and show how they interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ba2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import uuid\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner notebook output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install required packages if not already installed\n",
    "!pip install -q transformers sentence-transformers chromadb openai tiktoken\n",
    "\n",
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from openai import OpenAI\n",
    "\n",
    "# Configuration class for managing settings\n",
    "class Config:\n",
    "    \"\"\"Central configuration for the RAG system.\"\"\"\n",
    "    def __init__(self):\n",
    "        # OpenAI settings - you would normally use environment variables\n",
    "        self.openai_api_key = \"your_api_key_here\"  # Replace with actual key in production\n",
    "        self.openai_model = \"gpt-3.5-turbo\"  # Or gpt-4\n",
    "        \n",
    "        # Embedding settings\n",
    "        self.use_local_embeddings = True\n",
    "        self.embedding_model = \"all-MiniLM-L6-v2\"  # HuggingFace model\n",
    "        \n",
    "        # Vector DB settings\n",
    "        self.vector_db_path = tempfile.mkdtemp()  # Temporary directory for the database\n",
    "        \n",
    "        # Chunking settings\n",
    "        self.chunk_size = 500\n",
    "        self.chunk_overlap = 100\n",
    "        \n",
    "        # RAG settings\n",
    "        self.top_k_results = 3\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"Extracts and chunks text from PDF files.\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        # Initialize tokenizer for counting tokens\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "    def extract_text(self, pdf_text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"In a real implementation, this would use PyMuPDF (fitz) to extract text.\n",
    "        For this example, we'll simulate it with pre-formatted text.\"\"\"\n",
    "        # Simulate extraction: split by lines, each line is a page\n",
    "        return [\n",
    "            {\"content\": line, \"metadata\": {\"page\": i+1}}\n",
    "            for i, line in enumerate(pdf_text.strip().split('\\n'))\n",
    "        ]\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count the number of tokens in the text.\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def chunk_text(self, pages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split text into chunks based on token count.\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for page in pages:\n",
    "            text = page[\"content\"]\n",
    "            page_num = page[\"metadata\"][\"page\"]\n",
    "            \n",
    "            # For demo purposes, we'll use a simple approach\n",
    "            # In a real implementation, we would use a more sophisticated chunking algorithm\n",
    "            tokens = self.count_tokens(text)\n",
    "            \n",
    "            # If text is small enough, keep it as one chunk\n",
    "            if tokens <= self.config.chunk_size:\n",
    "                chunks.append({\n",
    "                    \"content\": text,\n",
    "                    \"metadata\": {\"page\": page_num, \"chunk_id\": f\"page_{page_num}_chunk_1\"}\n",
    "                })\n",
    "            else:\n",
    "                # Split by sentences (simple approximation)\n",
    "                sentences = text.split('. ')\n",
    "                current_chunk = \"\"\n",
    "                chunk_id = 1\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    # Add period back if it was removed by split\n",
    "                    if not sentence.endswith('.'):\n",
    "                        sentence += '.'\n",
    "                        \n",
    "                    # Check if adding this sentence would exceed chunk size\n",
    "                    potential_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "                    if self.count_tokens(potential_chunk) <= self.config.chunk_size:\n",
    "                        current_chunk = potential_chunk\n",
    "                    else:\n",
    "                        # Save current chunk if it's not empty\n",
    "                        if current_chunk:\n",
    "                            chunks.append({\n",
    "                                \"content\": current_chunk,\n",
    "                                \"metadata\": {\"page\": page_num, \"chunk_id\": f\"page_{page_num}_chunk_{chunk_id}\"}\n",
    "                            })\n",
    "                            chunk_id += 1\n",
    "                        current_chunk = sentence\n",
    "                \n",
    "                # Don't forget the last chunk\n",
    "                if current_chunk:\n",
    "                    chunks.append({\n",
    "                        \"content\": current_chunk,\n",
    "                        \"metadata\": {\"page\": page_num, \"chunk_id\": f\"page_{page_num}_chunk_{chunk_id}\"}\n",
    "                    })\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generates embeddings for text chunks using HuggingFace models.\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        # Initialize the embedding model\n",
    "        self.model = SentenceTransformer(config.embedding_model)\n",
    "        \n",
    "    def generate(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a single text.\"\"\"\n",
    "        return self.model.encode(text)\n",
    "    \n",
    "    def generate_batch(self, texts: List[str]) -> List[np.ndarray]:\n",
    "        \"\"\"Generate embeddings for a batch of texts.\"\"\"\n",
    "        return self.model.encode(texts)\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Stores and retrieves chunks using ChromaDB.\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        # Initialize ChromaDB client with persistent storage\n",
    "        self.client = chromadb.PersistentClient(path=config.vector_db_path)\n",
    "        \n",
    "        # Create a default collection with HuggingFace embedding function\n",
    "        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=config.embedding_model\n",
    "        )\n",
    "        \n",
    "        # Create or get the collection\n",
    "        try:\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=\"pdf_chunks\",\n",
    "                embedding_function=self.embedding_function\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing ChromaDB: {e}\")\n",
    "            # Fallback to creating a new collection\n",
    "            self.collection = self.client.create_collection(\n",
    "                name=f\"pdf_chunks_{uuid.uuid4().hex[:8]}\",\n",
    "                embedding_function=self.embedding_function\n",
    "            )\n",
    "    \n",
    "    def add(self, chunks: List[Dict[str, Any]]):\n",
    "        \"\"\"Add chunks to the vector store.\"\"\"\n",
    "        if not chunks:\n",
    "            return\n",
    "            \n",
    "        # Prepare data for insertion\n",
    "        ids = []\n",
    "        documents = []\n",
    "        metadatas = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"chunk_{i}_{uuid.uuid4().hex[:8]}\"\n",
    "            ids.append(chunk_id)\n",
    "            documents.append(chunk[\"content\"])\n",
    "            metadatas.append(chunk[\"metadata\"])\n",
    "        \n",
    "        # Add to collection\n",
    "        self.collection.add(\n",
    "            ids=ids,\n",
    "            documents=documents,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "        \n",
    "    def query(self, query_text: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Query the vector store for similar chunks.\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.config.top_k_results\n",
    "            \n",
    "        try:\n",
    "            results = self.collection.query(\n",
    "                query_texts=[query_text],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Format results\n",
    "            chunks = []\n",
    "            for i in range(len(results[\"ids\"][0])):\n",
    "                chunks.append({\n",
    "                    \"content\": results[\"documents\"][0][i],\n",
    "                    \"metadata\": results[\"metadatas\"][0][i]\n",
    "                })\n",
    "                \n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying vector store: {e}\")\n",
    "            return []\n",
    "\n",
    "class LLMService:\n",
    "    \"\"\"Interface with the OpenAI API.\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.client = None\n",
    "        if config.openai_api_key and config.openai_api_key != \"your_api_key_here\":\n",
    "            self.client = OpenAI(api_key=config.openai_api_key)\n",
    "    \n",
    "    def generate_answer(self, question: str, context: str) -> str:\n",
    "        \"\"\"Generate an answer using the OpenAI API.\"\"\"\n",
    "        # If no client is available, return a fallback answer\n",
    "        if not self.client:\n",
    "            return self._generate_fallback_answer(question, context)\n",
    "            \n",
    "        try:\n",
    "            prompt = self._build_prompt(question, context)\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.config.openai_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided document excerpts.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error with OpenAI API: {e}\")\n",
    "            return self._generate_fallback_answer(question, context)\n",
    "    \n",
    "    def _build_prompt(self, question: str, context: str) -> str:\n",
    "        \"\"\"Build a prompt for the LLM.\"\"\"\n",
    "        return f\"\"\"Answer the following question based ONLY on the provided context:\n",
    "        \n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer the question using ONLY information from the provided context.\n",
    "2. If the context doesn't contain the information needed, respond with 'I cannot answer this question based on the provided documents.'\n",
    "3. Cite the specific page numbers (e.g., [Page X]).\n",
    "4. Be concise and accurate.\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    def _generate_fallback_answer(self, question: str, context: str) -> str:\n",
    "        \"\"\"Generate a fallback answer when API is not available.\"\"\"\n",
    "        # Extract page numbers for citations\n",
    "        page_citations = set()\n",
    "        for line in context.split('\\n'):\n",
    "            if '[Page ' in line:\n",
    "                try:\n",
    "                    page = line.split('[Page ')[1].split(']')[0]\n",
    "                    page_citations.add(page)\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "        page_citations_str = \", \".join([f\"Page {p}\" for p in sorted(page_citations)])\n",
    "        \n",
    "        return f\"\"\"Based on the provided context, I found some information related to your question about '{question}'.\n",
    "        \n",
    "{context}\n",
    "\n",
    "The relevant information was found on {page_citations_str}.\n",
    "\n",
    "Note: This is a fallback answer as the OpenAI API is not configured. For more accurate answers, please provide an OpenAI API key.\"\"\"\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"Coordinates the RAG workflow.\"\"\"\n",
    "    def __init__(self, config: Optional[Config] = None):\n",
    "        # Initialize with default config if none provided\n",
    "        self.config = config or Config()\n",
    "        \n",
    "        # Initialize components with dependency injection\n",
    "        self.processor = PDFProcessor(self.config)\n",
    "        self.embedder = EmbeddingGenerator(self.config)\n",
    "        self.store = VectorStore(self.config)\n",
    "        self.llm = LLMService(self.config)\n",
    "    \n",
    "    def process_document(self, text: str) -> None:\n",
    "        \"\"\"Process a document and add it to the vector store.\"\"\"\n",
    "        print(\"Processing document...\")\n",
    "        # Extract text and create initial pages\n",
    "        pages = self.processor.extract_text(text)\n",
    "        \n",
    "        # Split into chunks\n",
    "        print(\"Splitting into chunks...\")\n",
    "        chunks = self.processor.chunk_text(pages)\n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Add to vector store\n",
    "        print(\"Adding chunks to vector store...\")\n",
    "        self.store.add(chunks)\n",
    "        print(\"Document processed and indexed successfully!\")\n",
    "    \n",
    "    def answer_question(self, question: str) -> Tuple[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Answer a question using the RAG pipeline.\"\"\"\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(\"Retrieving relevant context...\")\n",
    "        \n",
    "        # Retrieve relevant chunks\n",
    "        relevant_chunks = self.store.query(question)\n",
    "        \n",
    "        if not relevant_chunks:\n",
    "            return \"No relevant information found.\", []\n",
    "        \n",
    "        # Format context for the LLM\n",
    "        context = \"\\n\\n\".join([f\"[Page {c['metadata']['page']}]: {c['content']}\" for c in relevant_chunks])\n",
    "        \n",
    "        # Generate answer\n",
    "        print(\"Generating answer...\")\n",
    "        answer = self.llm.generate_answer(question, context)\n",
    "        \n",
    "        return answer, relevant_chunks\n",
    "\n",
    "# ---\n",
    "# Example usage and test\n",
    "config = Config()\n",
    "print(\"Initializing RAG pipeline...\")\n",
    "pipeline = RAGPipeline(config)\n",
    "\n",
    "# Sample PDF text (in a real scenario, we would extract this from a PDF)\n",
    "pdf_text = \"\"\"\n",
    "Page 1: The sun is the center of our solar system. It provides light and heat to the planets through nuclear fusion in its core. The temperature at the core of the sun reaches about 15 million degrees Celsius.\n",
    "\n",
    "Page 2: The Earth orbits the sun once every 365.25 days, which we call a year. This orbit is slightly elliptical rather than perfectly circular. The moon orbits the Earth approximately every 27.3 days, completing a full cycle of phases in about 29.5 days.\n",
    "\n",
    "Page 3: Solar energy can be converted into electricity using solar panels. This renewable energy source is becoming increasingly popular as technology improves and costs decrease. Solar panels work through the photovoltaic effect, where sunlight knocks electrons free from atoms, generating electricity.\n",
    "\"\"\"\n",
    "\n",
    "# Process document\n",
    "pipeline.process_document(pdf_text)\n",
    "\n",
    "# Test with questions\n",
    "questions = [\n",
    "    \"How does the Earth move around the sun?\",\n",
    "    \"What is the temperature at the core of the sun?\",\n",
    "    \"How do solar panels work?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer, chunks = pipeline.answer_question(question)\n",
    "    print(\"\\nRelevant chunks:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  {i}. [Page {chunk['metadata']['page']}] {chunk['content'][:100]}...\")\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(answer)\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "# ---\n",
    "# Design Rationale:\n",
    "# - Each class has a single responsibility and can be swapped or extended\n",
    "# - Real-world integrations: ChromaDB for vector storage, HuggingFace for embeddings, OpenAI for LLM\n",
    "# - Configuration is centralized and injectable\n",
    "# - The pipeline is testable and easy to maintain\n",
    "# - Error handling for robustness\n",
    "# - Fallbacks when external services aren't available\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
