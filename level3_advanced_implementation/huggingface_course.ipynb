{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc165a0",
   "metadata": {},
   "source": [
    "# Hugging Face: A Simple Course\n",
    "\n",
    "This notebook introduces Hugging Face's Transformers library, focusing on how to use it for text embeddings and language models in RAG applications. Hugging Face has become the de-facto standard for accessing state-of-the-art NLP models.\n",
    "\n",
    "## What is Hugging Face?\n",
    "\n",
    "Hugging Face is a company that provides:\n",
    "\n",
    "1. **The Transformers library**: An open-source library with pre-trained models\n",
    "2. **The Model Hub**: A platform for sharing and discovering models\n",
    "3. **The Datasets library**: A collection of NLP datasets\n",
    "4. **Spaces**: For deploying machine learning apps\n",
    "\n",
    "We'll focus mainly on the Transformers library and how to use it for generating embeddings and working with language models.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **Transformers**: The main library for working with pre-trained models\n",
    "- **Tokenizers**: For converting text to tokens that models can understand\n",
    "- **Pipelines**: Easy-to-use abstractions for common tasks\n",
    "- **Model Hub**: Where pre-trained models are shared and downloaded from\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba406bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the transformers library\n",
    "!pip install -q transformers sentence-transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574bf5fe",
   "metadata": {},
   "source": [
    "## Basic Usage: Text Classification\n",
    "\n",
    "Let's start with a simple example: using a pre-trained model to classify text sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb95f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text classification pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Analyze some text\n",
    "results = classifier([\n",
    "    \"I love using Hugging Face transformers!\",\n",
    "    \"This code is complicated and hard to understand.\",\n",
    "    \"Learning new libraries can be challenging but rewarding.\"\n",
    "])\n",
    "\n",
    "# Print the results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Text {i+1}: {result['label']} (Confidence: {result['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd71ade7",
   "metadata": {},
   "source": [
    "## Generating Embeddings with Sentence Transformers\n",
    "\n",
    "For RAG applications, we typically need to generate embeddings for documents and queries. Sentence Transformers, built on top of Hugging Face's Transformers, provides models specifically designed for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ff17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # A good, lightweight model\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"This is a sentence about artificial intelligence.\",\n",
    "    \"Embeddings are vector representations of text.\",\n",
    "    \"The weather today is sunny and warm.\",\n",
    "    \"Machine learning models can process natural language.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Print information about the embeddings\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Each sentence is represented by a {embeddings.shape[1]}-dimensional vector\")\n",
    "\n",
    "# Calculate similarity between sentences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"\\nSimilarity Matrix:\")\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        print(f\"Similarity between sentence {i+1} and {j+1}: {similarity_matrix[i][j]:.4f}\")\n",
    "    \n",
    "# Find the most similar pair\n",
    "max_sim = -1\n",
    "max_pair = (0, 0)\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j and similarity_matrix[i][j] > max_sim:\n",
    "            max_sim = similarity_matrix[i][j]\n",
    "            max_pair = (i, j)\n",
    "\n",
    "print(f\"\\nMost similar sentences: {max_pair[0]+1} and {max_pair[1]+1}\")\n",
    "print(f\"Sentence {max_pair[0]+1}: \\\"{sentences[max_pair[0]]}\\\"\")\n",
    "print(f\"Sentence {max_pair[1]+1}: \\\"{sentences[max_pair[1]]}\\\"\")\n",
    "print(f\"Similarity: {max_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1347aee",
   "metadata": {},
   "source": [
    "## Popular Embedding Models\n",
    "\n",
    "Hugging Face hosts numerous embedding models with different characteristics. Here are some popular ones:\n",
    "\n",
    "1. **all-MiniLM-L6-v2**: Small and fast, 384 dimensions\n",
    "2. **all-mpnet-base-v2**: Better quality but slower, 768 dimensions\n",
    "3. **all-distilroberta-v1**: Good balance of quality and speed\n",
    "4. **multi-qa-mpnet-base-dot-v1**: Specialized for question-answering\n",
    "\n",
    "Let's compare a couple of these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ae621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare embedding models\n",
    "models = [\n",
    "    'all-MiniLM-L6-v2',  # Fast, compact\n",
    "    'all-mpnet-base-v2'  # Higher quality\n",
    "]\n",
    "\n",
    "sentences = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Paris is the capital city of France.\",\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"France is a country in Western Europe.\"\n",
    "]\n",
    "\n",
    "import time\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nTesting model: {model_name}\")\n",
    "    \n",
    "    # Load model\n",
    "    start_time = time.time()\n",
    "    model = SentenceTransformer(model_name)\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"Model loading time: {load_time:.2f} seconds\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    start_time = time.time()\n",
    "    embeddings = model.encode(sentences)\n",
    "    encode_time = time.time() - start_time\n",
    "    print(f\"Encoding time for {len(sentences)} sentences: {encode_time:.4f} seconds\")\n",
    "    print(f\"Embedding dimensions: {embeddings.shape[1]}\")\n",
    "    \n",
    "    # Calculate similarity\n",
    "    sim = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Print similarity between question and each sentence\n",
    "    for i in range(1, len(sentences)):\n",
    "        print(f\"Similarity between question and sentence {i}: {sim[0][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599526e",
   "metadata": {},
   "source": [
    "## Using Transformers for Text Generation\n",
    "\n",
    "Besides embeddings, Hugging Face is widely used for text generation. Let's see how to use a language model to generate text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de269ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# We'll use a small model for demonstration purposes\n",
    "model_name = \"distilgpt2\"  # A smaller version of GPT-2\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Generate text\n",
    "def generate_text(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs.input_ids, \n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and return\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with some prompts\n",
    "prompts = [\n",
    "    \"Artificial intelligence is\",\n",
    "    \"The future of natural language processing\",\n",
    "    \"Transformers are neural networks that\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    response = generate_text(prompt)\n",
    "    print(f\"Generated: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5413c",
   "metadata": {},
   "source": [
    "## Pipelines: The Easy Way to Use Transformers\n",
    "\n",
    "Hugging Face provides pipelines that simplify many common tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0965ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Available tasks:\n",
    "tasks = [\n",
    "    \"sentiment-analysis\",\n",
    "    \"text-generation\",\n",
    "    \"text-classification\",\n",
    "    \"token-classification\",\n",
    "    \"question-answering\",\n",
    "    \"summarization\",\n",
    "    \"translation\",\n",
    "    \"feature-extraction\"  # For embeddings\n",
    "]\n",
    "\n",
    "print(\"Available pipeline tasks:\")\n",
    "for task in tasks:\n",
    "    print(f\"- {task}\")\n",
    "\n",
    "# Let's try a few\n",
    "\n",
    "# 1. Question answering\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "Hugging Face is an AI company that provides tools and libraries for natural language processing (NLP). \n",
    "Their most popular product is the Transformers library, which provides pre-trained models for a wide range of NLP tasks. \n",
    "The company was founded in 2016 and is based in New York City and Paris.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What is Hugging Face's most popular product?\",\n",
    "    \"Where is Hugging Face based?\",\n",
    "    \"When was Hugging Face founded?\"\n",
    "]\n",
    "\n",
    "print(\"\\nQuestion Answering:\")\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']} (Score: {result['score']:.4f})\")\n",
    "\n",
    "# 2. Summarization\n",
    "summarizer = pipeline(\"summarization\")\n",
    "long_text = \"\"\"\n",
    "Transformers is a deep learning architecture that has revolutionized natural language processing.\n",
    "It was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017.\n",
    "The key innovation was the self-attention mechanism, which allows the model to weigh the importance of different words in relation to each other.\n",
    "This has led to state-of-the-art results in translation, question answering, text generation, and many other NLP tasks.\n",
    "Pre-trained transformer models like BERT, GPT, and T5 have become the foundation for most advanced NLP systems.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nSummarization:\")\n",
    "summary = summarizer(long_text, max_length=50, min_length=10, do_sample=False)\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf787d9",
   "metadata": {},
   "source": [
    "## Working with Tokenizers\n",
    "\n",
    "To understand how transformers process text, it's important to understand tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Let's examine how different tokenizers process the same text\n",
    "tokenizer_names = [\n",
    "    \"bert-base-uncased\",  # WordPiece tokenizer\n",
    "    \"gpt2\",               # BPE tokenizer\n",
    "    \"google/t5-v1-base\"   # SentencePiece tokenizer\n",
    "]\n",
    "\n",
    "text = \"Hugging Face Transformers converts your text into tokens, which are numbers the model can understand.\"\n",
    "\n",
    "print(\"Tokenization example:\\n\")\n",
    "for name in tokenizer_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    \n",
    "    print(f\"Tokenizer: {name}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "    print(f\"First 10 tokens: {tokens[:10]}\")\n",
    "    print(f\"Token IDs: {token_ids[:10]}...\")\n",
    "    \n",
    "    # Decode back to text\n",
    "    decoded = tokenizer.decode(token_ids)\n",
    "    print(f\"Decoded: {decoded}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09295f9",
   "metadata": {},
   "source": [
    "## Integration with RAG Systems\n",
    "\n",
    "Now let's see how Hugging Face components can be integrated into a simple RAG (Retrieval Augmented Generation) system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4248c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self):\n",
    "        # Initialize embedding model\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Initialize generation model\n",
    "        self.generator = pipeline('text-generation', model='distilgpt2')\n",
    "        \n",
    "        # Knowledge base (documents and their embeddings)\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"Add documents to the knowledge base.\"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        \n",
    "        # Generate embeddings for new documents\n",
    "        new_embeddings = self.embedding_model.encode(documents)\n",
    "        \n",
    "        if len(self.embeddings) == 0:\n",
    "            self.embeddings = new_embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "    \n",
    "    def retrieve(self, query, top_k=2):\n",
    "        \"\"\"Retrieve most relevant documents for a query.\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "        \n",
    "        # Get top_k indices\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        return [(self.documents[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    def generate(self, query):\n",
    "        \"\"\"Generate an answer based on retrieved documents.\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.retrieve(query)\n",
    "        \n",
    "        # Format context\n",
    "        context = \"\\n\".join([doc for doc, _ in retrieved_docs])\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "        \n",
    "        # Generate answer\n",
    "        response = self.generator(prompt, max_length=len(prompt.split()) + 50, do_sample=True)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": response[0][\"generated_text\"].split(\"Answer:\")[1].strip(),\n",
    "            \"retrieved_documents\": retrieved_docs\n",
    "        }\n",
    "\n",
    "# Test the RAG system\n",
    "rag = SimpleRAG()\n",
    "\n",
    "# Add documents to knowledge base\n",
    "documents = [\n",
    "    \"The capital of France is Paris. Paris is known for the Eiffel Tower and Louvre Museum.\",\n",
    "    \"Rome is the capital of Italy. The Colosseum and Vatican City are in Rome.\",\n",
    "    \"Germany's capital is Berlin. Berlin is known for the Brandenburg Gate.\",\n",
    "    \"Madrid is the capital of Spain and home to the Prado Museum.\",\n",
    "    \"Tokyo is the capital of Japan and the most populous city in the world.\"\n",
    "]\n",
    "\n",
    "rag.add_documents(documents)\n",
    "\n",
    "# Test with a query\n",
    "query = \"What is the capital of France and what is it known for?\"\n",
    "result = rag.generate(query)\n",
    "\n",
    "print(\"Query:\", result[\"query\"])\n",
    "print(\"\\nRetrieved Documents:\")\n",
    "for i, (doc, score) in enumerate(result[\"retrieved_documents\"]):\n",
    "    print(f\"{i+1}. [{score:.4f}] {doc}\")\n",
    "\n",
    "print(\"\\nGenerated Answer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a4e3c",
   "metadata": {},
   "source": [
    "## Accessing the Hugging Face Hub\n",
    "\n",
    "The Hugging Face Hub hosts thousands of models and datasets. Let's see how to interact with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the huggingface_hub package if not already installed\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "from huggingface_hub import HfApi, list_models\n",
    "\n",
    "# Initialize the API\n",
    "api = HfApi()\n",
    "\n",
    "# List some models based on criteria\n",
    "def explore_models(task=None, library=None, limit=5):\n",
    "    models = list_models(\n",
    "        filter=task,\n",
    "        library=library,\n",
    "        limit=limit\n",
    "    )\n",
    "    \n",
    "    print(f\"Found models for task: {task}, library: {library}\")\n",
    "    for i, model_info in enumerate(models, 1):\n",
    "        print(f\"{i}. {model_info.modelId} (Downloads: {model_info.downloads:,})\")\n",
    "        \n",
    "# Explore models for different tasks\n",
    "print(\"Text embedding models:\")\n",
    "explore_models(task=\"feature-extraction\", limit=5)\n",
    "\n",
    "print(\"\\nText classification models:\")\n",
    "explore_models(task=\"text-classification\", limit=5)\n",
    "\n",
    "print(\"\\nQuestion answering models:\")\n",
    "explore_models(task=\"question-answering\", limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a64d8",
   "metadata": {},
   "source": [
    "## Best Practices for Using Hugging Face in RAG Systems\n",
    "\n",
    "1. **Choose the right embedding model**:\n",
    "   - For high quality: `all-mpnet-base-v2` or `all-distilroberta-v1`\n",
    "   - For speed: `all-MiniLM-L6-v2`\n",
    "   - For multilingual: `paraphrase-multilingual-mpnet-base-v2`\n",
    "\n",
    "2. **Effective text chunking**:\n",
    "   - Use semantic chunking when possible\n",
    "   - Consider sentence or paragraph boundaries\n",
    "   - Maintain appropriate context in each chunk\n",
    "\n",
    "3. **Model optimization**:\n",
    "   - Quantize models for faster inference\n",
    "   - Use smaller models when appropriate\n",
    "   - Consider batching embeddings generation\n",
    "\n",
    "4. **Prompt engineering**:\n",
    "   - Format retrieved context clearly\n",
    "   - Provide clear instructions in prompts\n",
    "   - Use examples in prompts when necessary\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Hugging Face provides powerful tools for working with state-of-the-art NLP models. Its libraries form the foundation of many modern RAG systems and other NLP applications. The combination of pre-trained models, easy-to-use APIs, and model sharing capabilities makes it an essential resource for anyone working with text data.\n",
    "\n",
    "For more information, visit the [Hugging Face documentation](https://huggingface.co/docs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
