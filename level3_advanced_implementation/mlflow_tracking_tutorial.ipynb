{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c3bd55b",
   "metadata": {},
   "source": [
    "# MLflow Tracking Tutorial\n",
    "\n",
    "This notebook provides an interactive introduction to MLflow tracking capabilities through both conceptual explanations and hands-on examples.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand MLflow's core tracking components\n",
    "- Configure and customize MLflow for experiment tracking\n",
    "- Track parameters, metrics, and artifacts for different model types\n",
    "- Implement hyperparameter tuning with MLflow\n",
    "- Access and interpret the MLflow UI\n",
    "- Apply best practices for model management\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to MLflow](#intro)\n",
    "2. [Setting Up the Environment](#setup)\n",
    "3. [MLflow Tracking Basics](#basics)\n",
    "4. [Tracking Scikit-learn Models](#sklearn)\n",
    "5. [Tracking TensorFlow Models](#tensorflow)\n",
    "6. [Hyperparameter Tuning](#tuning)\n",
    "7. [Working with the MLflow UI](#ui)\n",
    "8. [Best Practices](#practices)\n",
    "9. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a397d7c",
   "metadata": {},
   "source": [
    "## 1. Introduction to MLflow <a id=\"intro\"></a>\n",
    "\n",
    "MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It has four main components:\n",
    "\n",
    "1. **MLflow Tracking**: Records parameters, code versions, metrics, and artifacts\n",
    "2. **MLflow Projects**: Packages code in a reusable, reproducible form\n",
    "3. **MLflow Models**: Manages and deploys models from different ML libraries\n",
    "4. **MLflow Registry**: Centrally manages models through their lifecycle\n",
    "\n",
    "In this tutorial, we'll focus primarily on MLflow Tracking, which helps you:\n",
    "- Compare results between runs\n",
    "- Reproduce experiments\n",
    "- Share results with team members\n",
    "- Keep a permanent record of your experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9775288e",
   "metadata": {},
   "source": [
    "### MLflow Tracking Key Concepts\n",
    "\n",
    "* **Experiment**: A group of runs for a specific task\n",
    "* **Run**: A single execution of your code\n",
    "* **Parameters**: Key-value inputs to your code (e.g., hyperparameters)\n",
    "* **Metrics**: Key-value outputs from your code (e.g., accuracy, loss)\n",
    "* **Artifacts**: Files generated during the run (e.g., models, plots)\n",
    "* **Tags**: Additional metadata about the run\n",
    "\n",
    "Pseudocode for a basic MLflow workflow:\n",
    "\n",
    "```\n",
    "import mlflow\n",
    "\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(\"experiment_name\")\n",
    "\n",
    "# Start a run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"param_name\", value)\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"metric_name\", value)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc299f",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment <a id=\"setup\"></a>\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment. We'll use the existing utility functions from our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca8bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# Import utilities from our project\n",
    "from data_utils import load_dataset\n",
    "from model_utils import create_sklearn_model, create_tensorflow_model\n",
    "from mlflow_utils import load_config, setup_mlflow\n",
    "\n",
    "# Load configuration\n",
    "config = load_config('config.yaml')\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Dataset: {config['dataset']}\")\n",
    "print(f\"- Task: {config['task']}\")\n",
    "print(f\"- Model type: {config['model_type']}\")\n",
    "print(f\"- MLflow experiment: {config.get('mlflow', {}).get('experiment_name', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8fc89",
   "metadata": {},
   "source": [
    "### Understanding the Configuration File\n",
    "\n",
    "Our project uses a YAML configuration file to control experiments. The key sections are:\n",
    "\n",
    "1. **General configuration**: Dataset, task type, and optimization settings\n",
    "2. **MLflow configuration**: Tracking URI and experiment name\n",
    "3. **Model configuration**: Hyperparameters and model types for both sklearn and TensorFlow\n",
    "\n",
    "Let's examine a portion of this configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the MLflow section of the configuration\n",
    "import yaml\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"MLflow Configuration:\")\n",
    "print(yaml.dump(config.get('mlflow', {}), default_flow_style=False))\n",
    "\n",
    "print(\"\\nScikit-learn Configuration:\")\n",
    "print(yaml.dump(config.get('sklearn', {}), default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec19bda",
   "metadata": {},
   "source": [
    "### Setting Up MLflow Tracking\n",
    "\n",
    "Now let's set up MLflow tracking using our utility function. By default, MLflow will store runs in the local `./mlruns` directory, but you can also configure it to use a remote tracking server or database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MLflow\n",
    "from mlflow_utils import setup_mlflow\n",
    "setup_mlflow(config)\n",
    "\n",
    "# Check the tracking URI\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# List all experiments\n",
    "experiments = mlflow.search_experiments()\n",
    "print(\"\\nAvailable experiments:\")\n",
    "for exp in experiments:\n",
    "    print(f\"- {exp.name} (ID: {exp.experiment_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b2a4c8",
   "metadata": {},
   "source": [
    "## 3. MLflow Tracking Basics <a id=\"basics\"></a>\n",
    "\n",
    "Let's start with a simple example to understand the core concepts of MLflow tracking.\n",
    "\n",
    "In this example, we'll:\n",
    "1. Load a dataset\n",
    "2. Create a simple model\n",
    "3. Track parameters, metrics, and the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "X_train, X_test, y_train, y_test = load_dataset(\n",
    "    config['dataset'],\n",
    "    test_size=config.get('test_size', 0.2),\n",
    "    random_state=config.get('seed', 42)\n",
    ")\n",
    "\n",
    "# First tracking example\n",
    "with mlflow.start_run(run_name=\"basic_example\"):\n",
    "    # Log basic parameters\n",
    "    mlflow.log_param(\"dataset\", config['dataset'])\n",
    "    mlflow.log_param(\"test_size\", config.get('test_size', 0.2))\n",
    "    mlflow.log_param(\"random_state\", config.get('seed', 42))\n",
    "    \n",
    "    # Create a simple model (Decision Tree)\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "    \n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"model_type\", \"DecisionTree\")\n",
    "    mlflow.log_param(\"max_depth\", 3)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate and log metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        \"model\",\n",
    "        signature=mlflow.models.infer_signature(X_test, y_pred)\n",
    "    )\n",
    "    \n",
    "    # Log an artifact (feature importance plot)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(X_train.shape[1]), model.feature_importances_)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Importance')\n",
    "    \n",
    "    # Save the plot as a file\n",
    "    plt.savefig('feature_importance.png')\n",
    "    \n",
    "    # Log the plot as an artifact\n",
    "    mlflow.log_artifact('feature_importance.png')\n",
    "    \n",
    "    # Get the run ID for reference\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Clean up the plot file after logging\n",
    "os.remove('feature_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851244fc",
   "metadata": {},
   "source": [
    "### Tracking Components Explained\n",
    "\n",
    "Let's break down the key components of MLflow tracking:\n",
    "\n",
    "1. **Run**: The `mlflow.start_run()` context manager creates a new run in the current experiment. Each run represents one execution of your code.\n",
    "\n",
    "2. **Parameters**: We log parameters with `mlflow.log_param()`. Parameters are inputs that define how your model behaves (e.g., hyperparameters).\n",
    "\n",
    "3. **Metrics**: We log metrics with `mlflow.log_metric()`. Metrics are outputs that measure how well your model performs (e.g., accuracy).\n",
    "\n",
    "4. **Artifacts**: We log artifacts with `mlflow.log_artifact()`. Artifacts are files generated during the run (e.g., plots, datasets).\n",
    "\n",
    "5. **Model**: We log the model with `mlflow.sklearn.log_model()`. This stores the model in a format that can be loaded later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4d7e03",
   "metadata": {},
   "source": [
    "## 4. Tracking Scikit-learn Models <a id=\"sklearn\"></a>\n",
    "\n",
    "Let's create a more comprehensive example using scikit-learn models and our existing utility functions. We'll compare multiple runs with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fa115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and track a scikit-learn Random Forest model\n",
    "from mlflow_utils import log_sklearn_model\n",
    "from model_utils import create_sklearn_model\n",
    "\n",
    "# Define different parameter sets to try\n",
    "rf_params = [\n",
    "    {\"n_estimators\": 50, \"max_depth\": 10, \"random_state\": 42},\n",
    "    {\"n_estimators\": 100, \"max_depth\": 20, \"random_state\": 42},\n",
    "    {\"n_estimators\": 200, \"max_depth\": None, \"random_state\": 42}\n",
    "]\n",
    "\n",
    "# Compare multiple model configurations\n",
    "for i, params in enumerate(rf_params):\n",
    "    print(f\"\\nTraining Random Forest with params: {params}\")\n",
    "    \n",
    "    # Create the model\n",
    "    model = create_sklearn_model('random_forest', params, task=config['task'])\n",
    "    \n",
    "    # Log the model with MLflow\n",
    "    metrics = log_sklearn_model(model, X_train, X_test, y_train, y_test, params, config)\n",
    "    \n",
    "    print(f\"Results for run {i+1}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"- {metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d5703",
   "metadata": {},
   "source": [
    "### Understanding Autologging\n",
    "\n",
    "MLflow provides an autologging feature that automatically logs parameters, metrics, and models without requiring explicit logging statements. Let's enable autologging for scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autologging for scikit-learn\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Create a new model with autologging enabled\n",
    "with mlflow.start_run(run_name=\"sklearn_autolog_example\"):\n",
    "    # Log custom parameters not captured by autologging\n",
    "    mlflow.log_param(\"example_note\", \"Using autologging\")\n",
    "    \n",
    "    # Create and train the model\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    \n",
    "    # Autologging will log most metrics, but we can add custom ones\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    if len(np.unique(y_test)) == 2:  # Binary classification\n",
    "        y_proba = gb_model.predict_proba(X_test)[:,1]\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Disable autologging after use (optional)\n",
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f902034",
   "metadata": {},
   "source": [
    "## 5. Tracking TensorFlow Models <a id=\"tensorflow\"></a>\n",
    "\n",
    "Now let's track a TensorFlow model. TensorFlow models typically have different hyperparameters and training procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "from mlflow_utils import log_tensorflow_model\n",
    "\n",
    "# Define TensorFlow model parameters\n",
    "tf_params = {\n",
    "    \"units\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"activation\": \"relu\",\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 10,\n",
    "    \"patience\": 3,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "# Log the TensorFlow model\n",
    "print(\"Training TensorFlow model...\")\n",
    "metrics = log_tensorflow_model(tf_params, X_train, X_test, y_train, y_test, config)\n",
    "\n",
    "print(\"\\nTensorFlow model results:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"- {metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf91a414",
   "metadata": {},
   "source": [
    "### Autologging with TensorFlow\n",
    "\n",
    "TensorFlow's autologging captures:\n",
    "- Parameters from the model configuration\n",
    "- Metrics at each epoch during training\n",
    "- The computational graph\n",
    "- The final model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1629a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autologging for TensorFlow\n",
    "mlflow.tensorflow.autolog()\n",
    "\n",
    "# Create a new model with autologging enabled\n",
    "with mlflow.start_run(run_name=\"tensorflow_autolog_example\"):\n",
    "    # Create a simple model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid' if len(np.unique(y_train)) <= 2 else 'softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy' if len(np.unique(y_train)) <= 2 else 'sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Test the model\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Disable autologging after use (optional)\n",
    "mlflow.tensorflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8f6b0",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning with MLflow <a id=\"tuning\"></a>\n",
    "\n",
    "MLflow is particularly useful for hyperparameter tuning. Let's implement a simple hyperparameter search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from itertools import product\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Define a hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Create parameter combinations\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "print(f\"Testing {len(param_combinations)} hyperparameter combinations\")\n",
    "\n",
    "# Perform hyperparameter tuning with MLflow tracking\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f\"\\nTrial {i+1}/{len(param_combinations)}\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "    \n",
    "    # Start a run for this parameter combination\n",
    "    with mlflow.start_run(run_name=f\"dt_tuning_{i}\"):\n",
    "        # Log parameters\n",
    "        for param_name, param_value in params.items():\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "        \n",
    "        # Create and train the model\n",
    "        model = DecisionTreeClassifier(**params, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate and log metrics\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee31aeb",
   "metadata": {},
   "source": [
    "### Finding the Best Model\n",
    "\n",
    "After hyperparameter tuning, we can use MLflow to find the best model based on a specific metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85061131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best run using our utility function\n",
    "from mlflow_utils import find_best_run\n",
    "\n",
    "# Use the experiment name from config\n",
    "experiment_name = config.get('mlflow', {}).get('experiment_name', 'default_experiment')\n",
    "\n",
    "# Find the best run based on accuracy\n",
    "best_run = find_best_run(experiment_name, \"accuracy\", mode=\"max\")\n",
    "\n",
    "if best_run:\n",
    "    print(\"\\n=== Best Model ===\")\n",
    "    print(f\"Run ID: {best_run['run_id']}\")\n",
    "    print(f\"Accuracy: {best_run['metrics.accuracy']:.4f}\")\n",
    "    print(\"Parameters:\")\n",
    "    for key in best_run.keys():\n",
    "        if key.startswith('params.'):\n",
    "            param_name = key.replace('params.', '')\n",
    "            print(f\"  {param_name}: {best_run[key]}\")\n",
    "else:\n",
    "    print(\"No best model found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0de826",
   "metadata": {},
   "source": [
    "## 7. Working with the MLflow UI <a id=\"ui\"></a>\n",
    "\n",
    "MLflow provides a web-based user interface to visualize and compare experiments.\n",
    "\n",
    "To access the MLflow UI, run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "```\n",
    "\n",
    "Then open your browser to http://localhost:5000\n",
    "\n",
    "The MLflow UI allows you to:\n",
    "1. View all experiments\n",
    "2. Compare runs side-by-side\n",
    "3. Sort and filter runs based on parameters and metrics\n",
    "4. View model details and artifacts\n",
    "5. Download logged models\n",
    "6. Plot metrics over time\n",
    "\n",
    "Let's create a visualization similar to what you'd see in the UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90aa261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get runs for the current experiment to visualize\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "# Display a summary of the runs\n",
    "if not runs.empty:\n",
    "    print(f\"Found {len(runs)} runs in experiment '{experiment_name}'\")\n",
    "    \n",
    "    # Create a scatter plot of different runs\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Find columns with metrics\n",
    "    metric_cols = [col for col in runs.columns if col.startswith('metrics.')]\n",
    "    \n",
    "    if len(metric_cols) >= 2 and 'metrics.accuracy' in metric_cols:\n",
    "        # Choose two metrics to plot\n",
    "        metric_x = 'metrics.accuracy'\n",
    "        metric_y = metric_cols[1] if metric_cols[0] == 'metrics.accuracy' else metric_cols[0]\n",
    "        \n",
    "        plt.scatter(runs[metric_x], runs[metric_y])\n",
    "        plt.xlabel(metric_x.replace('metrics.', ''))\n",
    "        plt.ylabel(metric_y.replace('metrics.', ''))\n",
    "        plt.title('Comparison of Model Runs')\n",
    "        \n",
    "        # Add annotations for some points\n",
    "        for i, row in runs.iterrows():\n",
    "            if i % 3 == 0:  # Annotate every third point for clarity\n",
    "                plt.annotate(f\"Run {i}\", (row[metric_x], row[metric_y]))\n",
    "        \n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # Show metrics distribution\n",
    "    if 'metrics.accuracy' in metric_cols:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(runs['metrics.accuracy'], bins=10, alpha=0.7)\n",
    "        plt.title('Distribution of Accuracy Across Runs')\n",
    "        plt.xlabel('Accuracy')\n",
    "        plt.ylabel('Number of Runs')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"No runs found in experiment '{experiment_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8f63f",
   "metadata": {},
   "source": [
    "## 8. Best Practices for MLflow Tracking <a id=\"practices\"></a>\n",
    "\n",
    "Here are some best practices for effectively using MLflow:\n",
    "\n",
    "1. **Organize experiments logically**: Create separate experiments for different objectives or datasets.\n",
    "\n",
    "2. **Log all relevant parameters**: Document everything needed to reproduce your experiment.\n",
    "\n",
    "3. **Use descriptive run names**: Name your runs to easily identify them later.\n",
    "\n",
    "4. **Log model signatures**: Include input/output signatures for better model serving.\n",
    "\n",
    "5. **Version your data**: Track which version of the data was used for training.\n",
    "\n",
    "6. **Use tags for additional metadata**: Add tags to categorize runs and add context.\n",
    "\n",
    "7. **Set up a persistent backend**: Use a database backend for production environments.\n",
    "\n",
    "8. **Log environment details**: Record package versions and environment configurations.\n",
    "\n",
    "Let's implement some of these best practices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example implementing best practices\n",
    "with mlflow.start_run(run_name=\"best_practices_example\"):\n",
    "    # 1. Log all relevant parameters\n",
    "    mlflow.log_param(\"dataset\", config['dataset'])\n",
    "    mlflow.log_param(\"test_size\", config.get('test_size', 0.2))\n",
    "    mlflow.log_param(\"model_type\", \"DecisionTree\")\n",
    "    mlflow.log_param(\"max_depth\", 5)\n",
    "    \n",
    "    # 2. Add tags for additional metadata\n",
    "    mlflow.set_tag(\"version\", \"v1.0.0\")\n",
    "    mlflow.set_tag(\"author\", \"MLflow Tutorial\")\n",
    "    mlflow.set_tag(\"purpose\", \"Educational\")\n",
    "    mlflow.set_tag(\"priority\", \"high\")\n",
    "    \n",
    "    # 3. Log environment details\n",
    "    import sys\n",
    "    mlflow.log_param(\"python_version\", sys.version)\n",
    "    mlflow.log_param(\"scikit_learn_version\", pd.__version__)\n",
    "    \n",
    "    # 4. Create and train a model\n",
    "    model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 5. Log metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # 6. Log the model with signature and input example\n",
    "    signature = mlflow.models.infer_signature(X_test, y_pred)\n",
    "    input_example = X_test[:5]\n",
    "    \n",
    "    mlflow.sklearn.log_model(\n",
    "        model,\n",
    "        \"model\",\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )\n",
    "    \n",
    "    # 7. Log a dataset info artifact\n",
    "    dataset_info = {\n",
    "        \"name\": config['dataset'],\n",
    "        \"n_samples\": len(X_train) + len(X_test),\n",
    "        \"n_features\": X_train.shape[1],\n",
    "        \"date_processed\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    # Write dataset info to a file\n",
    "    with open(\"dataset_info.json\", \"w\") as f:\n",
    "        import json\n",
    "        json.dump(dataset_info, f)\n",
    "    \n",
    "    # Log the file as an artifact\n",
    "    mlflow.log_artifact(\"dataset_info.json\")\n",
    "    \n",
    "    print(f\"Run completed with accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "# Clean up\n",
    "os.remove(\"dataset_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f809632",
   "metadata": {},
   "source": [
    "## 9. Exercises <a id=\"exercises\"></a>\n",
    "\n",
    "Now let's practice what we've learned with some exercises. Try to complete these on your own before looking at the solutions.\n",
    "\n",
    "### Exercise 1: Compare Multiple Scikit-learn Classifiers\n",
    "\n",
    "Create an experiment that compares different scikit-learn classifiers (Decision Tree, Random Forest, Logistic Regression) on the same dataset.\n",
    "\n",
    "1. Log each model type with its default parameters\n",
    "2. Record accuracy, precision, recall, and F1 score for each\n",
    "3. Create a visual comparison of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a58e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Solution\n",
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Track results for comparison\n",
    "results = {}\n",
    "\n",
    "# Compare models with MLflow\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"compare_{name}\"):\n",
    "        # Log model type\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "            \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "            \"f1_score\": f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Store results for visualization\n",
    "        results[name] = metrics\n",
    "        \n",
    "        print(f\"{name} - Accuracy: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "# Create a visualization\n",
    "metrics_df = pd.DataFrame(results).transpose()\n",
    "metrics_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Classifier Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel comparison:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2acf52",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Cross-Validation with MLflow\n",
    "\n",
    "Implement k-fold cross-validation while tracking each fold's performance with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575824fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Solution\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the model\n",
    "model_type = 'RandomForest'\n",
    "base_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Start a parent run\n",
    "with mlflow.start_run(run_name=f\"cv_{model_type}\"):\n",
    "    # Log parent run parameters\n",
    "    mlflow.log_param(\"model_type\", model_type)\n",
    "    mlflow.log_param(\"n_splits\", n_splits)\n",
    "    mlflow.log_param(\"dataset\", config['dataset'])\n",
    "    \n",
    "    # Store fold results\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    # Combine X_train and X_test for full dataset CV\n",
    "    X_full = np.vstack((X_train, X_test))\n",
    "    y_full = np.concatenate((y_train, y_test))\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_full)):\n",
    "        print(f\"\\nTraining fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        # Get fold data\n",
    "        X_fold_train, X_fold_val = X_full[train_idx], X_full[val_idx]\n",
    "        y_fold_train, y_fold_val = y_full[train_idx], y_full[val_idx]\n",
    "        \n",
    "        # Create a nested run for this fold\n",
    "        with mlflow.start_run(run_name=f\"fold_{fold+1}\", nested=True):\n",
    "            # Log fold info\n",
    "            mlflow.log_param(\"fold\", fold+1)\n",
    "            \n",
    "            # Train model on fold\n",
    "            model = clone(base_model)  # Clone to get a fresh model\n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            y_pred = model.predict(X_fold_val)\n",
    "            accuracy = accuracy_score(y_fold_val, y_pred)\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            \n",
    "            # Store for averaging\n",
    "            fold_accuracies.append(accuracy)\n",
    "            \n",
    "            print(f\"Fold {fold+1} accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Log average accuracy in parent run\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    std_accuracy = np.std(fold_accuracies)\n",
    "    mlflow.log_metric(\"mean_accuracy\", avg_accuracy)\n",
    "    mlflow.log_metric(\"std_accuracy\", std_accuracy)\n",
    "    \n",
    "    print(f\"\\nCross-validation results:\")\n",
    "    print(f\"Mean accuracy: {avg_accuracy:.4f} ± {std_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc4bd1f",
   "metadata": {},
   "source": [
    "### Exercise 3: Create a Custom Artifact Visualization\n",
    "\n",
    "Create a custom visualization of your model results and log it as an MLflow artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d96823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Solution\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Train a model for visualization\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"visualization_example\"):\n",
    "    # Log basic metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # 1. Create and log confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Check if binary or multiclass\n",
    "    classes = np.unique(y_test)\n",
    "    \n",
    "    # Use a colormap\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Add labels\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save and log the figure\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "    \n",
    "    # 2. Feature importance visualization\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Sort features by importance\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Plot the feature importances\n",
    "        plt.bar(range(min(10, len(importances))), importances[indices][:10])\n",
    "        plt.title('Top 10 Feature Importances')\n",
    "        plt.xticks(range(min(10, len(importances))), indices[:10])\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save and log the figure\n",
    "        plt.savefig(\"feature_importance.png\")\n",
    "        mlflow.log_artifact(\"feature_importance.png\")\n",
    "    \n",
    "    # 3. ROC curve for binary classification\n",
    "    if len(classes) == 2:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        y_proba_positive = y_proba[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba_positive)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Save and log the figure\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        mlflow.log_artifact(\"roc_curve.png\")\n",
    "        \n",
    "        # Also log the AUC as a metric\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(model, \"visualization_model\")\n",
    "    \n",
    "    print(f\"Visualizations created and logged to MLflow\")\n",
    "\n",
    "# Clean up\n",
    "os.remove(\"confusion_matrix.png\")\n",
    "if os.path.exists(\"feature_importance.png\"):\n",
    "    os.remove(\"feature_importance.png\")\n",
    "if os.path.exists(\"roc_curve.png\"):\n",
    "    os.remove(\"roc_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973575d0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored MLflow's tracking capabilities for machine learning experiment management:\n",
    "\n",
    "1. Setting up MLflow for experiment tracking\n",
    "2. Logging parameters, metrics, and artifacts\n",
    "3. Tracking different model types (scikit-learn and TensorFlow)\n",
    "4. Performing hyperparameter tuning with MLflow\n",
    "5. Finding the best model based on metrics\n",
    "6. Creating visualizations and custom artifacts\n",
    "7. Applying best practices for experiment management\n",
    "\n",
    "MLflow provides a structured approach to experiment tracking that helps you:\n",
    "- Compare results across multiple runs\n",
    "- Ensure reproducibility of experiments\n",
    "- Share results with team members\n",
    "- Deploy models to production\n",
    "\n",
    "To learn more, visit the [MLflow documentation](https://mlflow.org/docs/latest/index.html)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
