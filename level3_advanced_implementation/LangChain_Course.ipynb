{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c8cede7",
   "metadata": {},
   "source": [
    "# LangChain: From Zero to Hero\n",
    "\n",
    "Welcome to this comprehensive LangChain course! By the end of this tutorial, you'll have a solid understanding of how to build powerful applications with Large Language Models (LLMs) using the LangChain framework.\n",
    "\n",
    "## Course Outline\n",
    "\n",
    "1. Introduction to LangChain\n",
    "2. Setting Up Your Environment\n",
    "3. LangChain Components\n",
    "   - Models\n",
    "   - Prompts\n",
    "   - Memory\n",
    "   - Chains\n",
    "   - Agents\n",
    "   - Tools\n",
    "4. Building Simple Applications\n",
    "5. Advanced Use Cases\n",
    "6. Best Practices\n",
    "7. Project Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d54dd84",
   "metadata": {},
   "source": [
    "## 1. Introduction to LangChain\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. It enables the creation of applications that are:\n",
    "- **Data-aware**: connect LLMs to other data sources\n",
    "- **Agentic**: allow LLMs to interact with their environment\n",
    "\n",
    "### Why LangChain?\n",
    "\n",
    "- Simplifies integration with various LLMs (OpenAI, Anthropic, Hugging Face, etc.)\n",
    "- Provides components for building complex chains and agents\n",
    "- Offers tools for enhancing LLM capabilities\n",
    "- Enables memory management for contextual conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa2b5d",
   "metadata": {},
   "source": [
    "## 2. Setting Up Your Environment\n",
    "\n",
    "Let's start by installing the required packages and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f762caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/hatem/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting langchain\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.79.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: chromadb in /home/hatem/.local/lib/python3.10/site-packages (0.4.22)\n",
      "Requirement already satisfied: tiktoken in /home/hatem/.local/lib/python3.10/site-packages (0.9.0)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: python-dotenv in /home/hatem/.local/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /home/hatem/.local/lib/python3.10/site-packages (from langchain) (0.3.58)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/hatem/.local/lib/python3.10/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/hatem/.local/lib/python3.10/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/hatem/.local/lib/python3.10/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/hatem/.local/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/hatem/.local/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/hatem/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/hatem/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/hatem/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/hatem/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/hatem/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/hatem/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/hatem/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/hatem/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/hatem/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /home/hatem/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: certifi in /home/hatem/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/hatem/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.3)\n",
      "Requirement already satisfied: h11>=0.16 in /home/hatem/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/hatem/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/hatem/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hatem/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hatem/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/hatem/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /home/hatem/.local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/hatem/.local/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/hatem/.local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (0.115.11)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /home/hatem/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (3.6.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (1.16.3)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (1.68.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/hatem/.local/lib/python3.10/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/hatem/.local/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: pyproject_hooks in /home/hatem/.local/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/hatem/.local/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /home/hatem/.local/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.46.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/hatem/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/hatem/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.37.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/hatem/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/hatem/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/hatem/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/hatem/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/hatem/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/hatem/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/hatem/.local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/hatem/.local/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in /home/hatem/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/hatem/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /home/hatem/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.7)\n",
      "Requirement already satisfied: sympy in /home/hatem/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/lib/python3/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.13)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /home/hatem/.local/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/hatem/.local/lib/python3.10/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/hatem/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /home/hatem/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.1 in /home/hatem/.local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /home/hatem/.local/lib/python3.10/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /home/hatem/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /home/hatem/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /home/hatem/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/lib/python3/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.13.3)\n",
      "Requirement already satisfied: asgiref~=3.0 in /home/hatem/.local/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/hatem/.local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/hatem/.local/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
      "Requirement already satisfied: filelock in /home/hatem/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hatem/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/hatem/.local/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/hatem/.local/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/hatem/.local/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (13.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/hatem/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/hatem/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/hatem/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/hatem/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/hatem/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/hatem/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/hatem/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/hatem/.local/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hatem/.local/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m400.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading openai-1.79.0-py3-none-any.whl (683 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m268.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m268.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/hatem/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: protobuf, jiter, bs4, async-timeout, openai, langchain-text-splitters, langchain\n",
      "\u001b[2K  Attempting uninstall: protobuf\n",
      "\u001b[2K    Found existing installation: protobuf 4.25.7\n",
      "\u001b[2K    Uninstalling protobuf-4.25.7:\n",
      "\u001b[2K      Successfully uninstalled protobuf-4.25.7\n",
      "\u001b[2K  Attempting uninstall: async-timeout[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [bs4]m [jiter]\n",
      "\u001b[2K    Found existing installation: async-timeout 5.0.1m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/7\u001b[0m [async-timeout]\n",
      "\u001b[2K    Uninstalling async-timeout-5.0.1:m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/7\u001b[0m [async-timeout]\n",
      "\u001b[2K      Successfully uninstalled async-timeout-5.0.1\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/7\u001b[0m [async-timeout]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [langchain]━\u001b[0m \u001b[32m6/7\u001b[0m [langchain]gchain-text-splitters]\n",
      "\u001b[1A\u001b[2K\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/hatem/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.18 requires protobuf<5,>=4.25.3, but you have protobuf 5.29.4 which is incompatible.\n",
      "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
      "tflite-support 0.4.4 requires protobuf<4,>=3.18.0, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed async-timeout-4.0.3 bs4-0.0.2 jiter-0.10.0 langchain-0.3.25 langchain-text-splitters-0.3.8 openai-1.79.0 protobuf-5.29.4\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install langchain openai chromadb tiktoken bs4 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc6965",
   "metadata": {},
   "source": [
    "Before proceeding, you'll need to set up your API keys. For this course, we'll use OpenAI's models, but LangChain supports many other providers.\n",
    "\n",
    "Create a `.env` file in your project directory with:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "```\n",
    "\n",
    "Let's load the environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Check if the API key is loaded\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OpenAI API key loaded successfully!\")\n",
    "else:\n",
    "    print(\"OpenAI API key not found. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd995db",
   "metadata": {},
   "source": [
    "## 3. LangChain Components\n",
    "\n",
    "LangChain provides several key components that you can combine to create powerful applications. Let's explore each one.\n",
    "\n",
    "### 3.1 Models\n",
    "\n",
    "LangChain supports various types of models:\n",
    "- Language models (LLMs)\n",
    "- Chat models\n",
    "- Text embedding models\n",
    "\n",
    "Let's start with a simple example using an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adfa709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Ask a question\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8f755",
   "metadata": {},
   "source": [
    "Now, let's try a chat model, which is designed for conversational interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e75b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Initialize the chat model\n",
    "chat = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "# Create a simple conversation\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hello! Can you tell me about LangChain?\")\n",
    "]\n",
    "\n",
    "# Get a response\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b585d",
   "metadata": {},
   "source": [
    "### 3.2 Prompts\n",
    "\n",
    "Prompts are structured inputs to language models. LangChain provides tools for managing prompts:\n",
    "\n",
    "- **PromptTemplates**: Create dynamic templates with variables\n",
    "- **Few-shot examples**: Include examples in prompts\n",
    "- **OutputParsers**: Structure the output from LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Simple prompt template\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Format the prompt with actual values\n",
    "formatted_prompt = prompt.format(\n",
    "    context=\"Paris is the capital and most populous city of France.\",\n",
    "    question=\"What is the capital of France?\"\n",
    ")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the formatted prompt with our LLM\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb12ff",
   "metadata": {},
   "source": [
    "Let's try a more complex example with few-shot learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94de724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Define our examples\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "    {\"word\": \"big\", \"antonym\": \"small\"}\n",
    "]\n",
    "\n",
    "# Create an example template\n",
    "example_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Create a few-shot prompt template\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of the following word:\",\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Format the prompt with our input\n",
    "formatted_prompt = few_shot_prompt.format(input=\"hot\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the formatted prompt with our LLM\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0fd585",
   "metadata": {},
   "source": [
    "### 3.3 Memory\n",
    "\n",
    "Memory components allow LLMs to retain context across interactions. This is crucial for building conversational applications.\n",
    "\n",
    "Let's implement a simple conversation with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a02c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Initialize the conversation with memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "# First interaction\n",
    "response1 = conversation.invoke(\"My name is Alex.\")\n",
    "print(response1[\"response\"])\n",
    "\n",
    "# Second interaction (the model should remember the name)\n",
    "response2 = conversation.invoke(\"What's my name?\")\n",
    "print(response2[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d8db4",
   "metadata": {},
   "source": [
    "Let's explore different types of memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Initialize with window memory (only remembers last k interactions)\n",
    "conversation_window = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferWindowMemory(k=2)\n",
    ")\n",
    "\n",
    "# Conversation with limited memory\n",
    "response1 = conversation_window.invoke(\"Hi, I'm Sarah.\")\n",
    "print(\"Response 1:\", response1[\"response\"])\n",
    "\n",
    "response2 = conversation_window.invoke(\"I live in New York.\")\n",
    "print(\"Response 2:\", response2[\"response\"])\n",
    "\n",
    "response3 = conversation_window.invoke(\"I work as a software engineer.\")\n",
    "print(\"Response 3:\", response3[\"response\"])\n",
    "\n",
    "# This should only remember the last 2 interactions, not the first one\n",
    "response4 = conversation_window.invoke(\"Where do I live and what's my job?\")\n",
    "print(\"Response 4:\", response4[\"response\"])\n",
    "\n",
    "response5 = conversation_window.invoke(\"What's my name?\")  # Should not remember\n",
    "print(\"Response 5:\", response5[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e85e49",
   "metadata": {},
   "source": [
    "### 3.4 Chains\n",
    "\n",
    "Chains combine multiple components to create a specific sequence of operations. Let's look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4243dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a simple chain that formats a prompt and passes it to an LLM\n",
    "template = \"What is the capital of {country}?\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"country\"])\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke(\"France\")\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5572cd1",
   "metadata": {},
   "source": [
    "Let's create a more complex chain that combines multiple steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain, LLMChain\n",
    "\n",
    "# First chain: Generate a short story about a topic\n",
    "template1 = \"Write a very short story about {topic} in 3 sentences.\"\n",
    "prompt_template1 = PromptTemplate(template=template1, input_variables=[\"topic\"])\n",
    "story_chain = LLMChain(llm=llm, prompt=prompt_template1, output_key=\"story\")\n",
    "\n",
    "# Second chain: Generate a title for the story\n",
    "template2 = \"Create a title for this story: {story}\"\n",
    "prompt_template2 = PromptTemplate(template=template2, input_variables=[\"story\"])\n",
    "title_chain = LLMChain(llm=llm, prompt=prompt_template2, output_key=\"title\")\n",
    "\n",
    "# Combine the chains\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[story_chain, title_chain],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"story\", \"title\"]\n",
    ")\n",
    "\n",
    "# Run the combined chain\n",
    "response = sequential_chain.invoke(\"a magical forest\")\n",
    "print(\"Title:\", response[\"title\"])\n",
    "print(\"\\nStory:\", response[\"story\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70e31f",
   "metadata": {},
   "source": [
    "### 3.5 Agents\n",
    "\n",
    "Agents use LLMs to determine which actions to take and in what order. They combine LLMs with tools to interact with external systems.\n",
    "\n",
    "Let's create a simple agent that can perform calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90435a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.tools import Tool\n",
    "\n",
    "# Load tools\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\"What is the square root of 4 plus the square root of 9?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398731ba",
   "metadata": {},
   "source": [
    "Let's create a more complex agent that can search the internet and perform calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a custom tool\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "# You would need to set up Google Search API keys for this\n",
    "# search = GoogleSearchAPIWrapper()\n",
    "\n",
    "# For demo purposes, let's create a dummy search tool\n",
    "def dummy_search(query):\n",
    "    if \"weather\" in query.lower():\n",
    "        return \"The weather is sunny and 75 degrees.\"\n",
    "    elif \"population\" in query.lower():\n",
    "        return \"The population of the mentioned location is approximately 3 million.\"\n",
    "    else:\n",
    "        return \"I found several websites related to your query.\"\n",
    "\n",
    "# Create our tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=dummy_search,\n",
    "        description=\"Useful for when you need to search for information on the internet.\"\n",
    "    ),\n",
    "    *load_tools([\"llm-math\"], llm=llm)\n",
    "]\n",
    "\n",
    "# Initialize the agent with our tools\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the agent with a complex query\n",
    "agent.invoke(\"What's the weather like today? If the temperature in Celsius is the current temperature minus 32 divided by 1.8, what would it be in Celsius?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d53b65",
   "metadata": {},
   "source": [
    "### 3.6 Tools\n",
    "\n",
    "Tools allow language models to interact with external systems. LangChain provides many built-in tools, and you can create custom ones.\n",
    "\n",
    "Let's create a custom tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from typing import Optional, Type\n",
    "\n",
    "class CalculatorTool(BaseTool):\n",
    "    name = \"Calculator\"\n",
    "    description = \"Useful for performing basic arithmetic operations\"\n",
    "    \n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            result = eval(query)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        # For async implementation\n",
    "        raise NotImplementedError(\"CalculatorTool does not support async\")\n",
    "\n",
    "# Create an instance of our custom tool\n",
    "calculator_tool = CalculatorTool()\n",
    "\n",
    "# Create an agent with our custom tool\n",
    "tools = [calculator_tool]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test the agent with an arithmetic query\n",
    "agent.invoke(\"What is 123 * 456?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b847849",
   "metadata": {},
   "source": [
    "## 4. Building Simple Applications\n",
    "\n",
    "Now that we understand the core components, let's build some simple applications.\n",
    "\n",
    "### 4.1 Question-Answering over Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3cdd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# First, let's create a sample document\n",
    "with open(\"sample_document.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "It enables applications that are context-aware and reason-driven.\n",
    "LangChain provides modules for working with language models, prompts, memory, indexes, chains and agents.\n",
    "The framework is designed to be modular and extensible, making it easy to build complex applications.\n",
    "LangChain was developed to make it easier for developers to create applications using large language models.\n",
    "It provides a standard interface for connecting language models to other data sources and allowing language models to interact with their environment.\n",
    "One of the key features of LangChain is its ability to chain together multiple components to create more complex applications.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "# Load the document\n",
    "loader = TextLoader(\"sample_document.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings and store in vector database\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(chunks, embeddings)\n",
    "\n",
    "# Create a retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "# Ask questions\n",
    "question = \"What is LangChain used for?\"\n",
    "response = qa_chain.invoke(question)\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d186bb",
   "metadata": {},
   "source": [
    "### 4.2 Building a Chatbot with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de13170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create a conversational retrieval chain\n",
    "retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# First question\n",
    "response = retrieval_chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "print(\"Response 1:\", response[\"answer\"])\n",
    "\n",
    "# Follow-up question (should use context from previous question)\n",
    "response = retrieval_chain.invoke({\"question\": \"What are its key features?\"})\n",
    "print(\"Response 2:\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b83f2",
   "metadata": {},
   "source": [
    "## 5. Advanced Use Cases\n",
    "\n",
    "Now let's explore some more advanced applications of LangChain.\n",
    "\n",
    "### 5.1 Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# Let's create a longer text for summarization\n",
    "with open(\"article.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "Artificial Intelligence (AI) is revolutionizing industries across the globe. From healthcare to finance, transportation to entertainment, AI is transforming how businesses operate and how people live their daily lives.\n",
    "\n",
    "In healthcare, AI systems are being used to diagnose diseases, analyze medical images, and develop personalized treatment plans. These technologies can process vast amounts of medical data far more quickly than human doctors, potentially leading to earlier diagnoses and better patient outcomes.\n",
    "\n",
    "The financial sector has embraced AI for fraud detection, algorithmic trading, and customer service. AI-powered chatbots now handle routine customer inquiries, freeing up human representatives for more complex issues. Meanwhile, sophisticated algorithms analyze market trends and execute trades at speeds impossible for human traders.\n",
    "\n",
    "Transportation is being revolutionized by autonomous vehicle technology. Companies like Tesla, Waymo, and Uber are investing heavily in self-driving cars, promising to reduce accidents, ease traffic congestion, and provide mobility options for those unable to drive.\n",
    "\n",
    "In the entertainment industry, streaming services like Netflix and Spotify use AI to analyze user preferences and recommend content. These recommendation engines have transformed how people discover new movies, shows, and music.\n",
    "\n",
    "Despite these advancements, AI raises important ethical considerations. Issues of privacy, bias in algorithms, job displacement, and the need for regulatory frameworks are ongoing challenges that society must address as AI technology continues to evolve.\n",
    "\n",
    "As we look to the future, the integration of AI into everyday life is likely to accelerate. Those businesses and individuals who adapt to this changing technological landscape will be best positioned to thrive in an increasingly AI-driven world.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "# Load the document\n",
    "loader = TextLoader(\"article.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Create a summarization chain\n",
    "summarize_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "# Generate summary\n",
    "summary = summarize_chain.invoke(documents)\n",
    "print(summary[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba0061",
   "metadata": {},
   "source": [
    "### 5.2 Creating an Agent with Multiple Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "# Create several custom tools\n",
    "\n",
    "class WeatherTool(BaseTool):\n",
    "    name = \"Weather\"\n",
    "    description = \"Get current weather in a location\"\n",
    "    \n",
    "    def _run(self, location: str) -> str:\n",
    "        # In a real scenario, you would call a weather API\n",
    "        return f\"The weather in {location} is currently sunny and 75°F (24°C).\"\n",
    "    \n",
    "    def _arun(self, location: str):\n",
    "        raise NotImplementedError(\"WeatherTool does not support async\")\n",
    "\n",
    "class NewsTool(BaseTool):\n",
    "    name = \"News\"\n",
    "    description = \"Get latest news on a topic\"\n",
    "    \n",
    "    def _run(self, topic: str) -> str:\n",
    "        # In a real scenario, you would call a news API\n",
    "        return f\"Latest news on {topic}: New developments have been reported recently.\"\n",
    "    \n",
    "    def _arun(self, topic: str):\n",
    "        raise NotImplementedError(\"NewsTool does not support async\")\n",
    "\n",
    "# Initialize our tools\n",
    "weather_tool = WeatherTool()\n",
    "news_tool = NewsTool()\n",
    "calculator_tool = CalculatorTool()  # From earlier example\n",
    "\n",
    "# Create an agent with all our tools\n",
    "tools = [weather_tool, news_tool, calculator_tool]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test the agent with a complex query\n",
    "agent.invoke(\"What's the weather like in Paris? Also, can you tell me the latest news on AI? Finally, what is 17 * 38?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7c8ebd",
   "metadata": {},
   "source": [
    "### 5.3 Document Question-Answering with Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "# Create a QA chain that returns sources\n",
    "qa_with_sources_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "response = qa_with_sources_chain.invoke({\"question\": \"What are the key features of LangChain?\"})\n",
    "print(\"Answer:\", response[\"answer\"])\n",
    "print(\"\\nSources:\", response[\"sources\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ccffaa",
   "metadata": {},
   "source": [
    "## 6. Best Practices\n",
    "\n",
    "Here are some best practices for working with LangChain:\n",
    "\n",
    "### 6.1 Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ef770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a well-structured prompt template\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "good_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert {profession}.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Based on the context provided, please answer the following question in a {tone} tone. \n",
    "Be {detail_level} in your response.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\",\n",
    "    input_variables=[\"profession\", \"context\", \"tone\", \"detail_level\", \"question\"]\n",
    ")\n",
    "\n",
    "formatted_prompt = good_prompt.format(\n",
    "    profession=\"data scientist\",\n",
    "    context=\"This is a dataset of customer purchase history over 5 years.\",\n",
    "    tone=\"professional\",\n",
    "    detail_level=\"detailed\",\n",
    "    question=\"What trends can we identify in this data?\"\n",
    ")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7309e8f",
   "metadata": {},
   "source": [
    "### 6.2 Handling Rate Limits and Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "# Example function showing how to handle rate limits\n",
    "def safe_llm_call(llm, prompt, max_retries=3):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return llm.invoke(prompt)\n",
    "        except Exception as e:\n",
    "            if \"rate limit\" in str(e).lower():\n",
    "                wait_time = (2 ** retries) + random.random()  # Exponential backoff\n",
    "                print(f\"Rate limit hit. Waiting {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                return \"An error occurred while processing your request.\"\n",
    "    \n",
    "    return \"Maximum retries exceeded. Please try again later.\"\n",
    "\n",
    "# Test with a simple prompt\n",
    "result = safe_llm_call(llm, \"Tell me a short joke about programming\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db95777",
   "metadata": {},
   "source": [
    "### 6.3 Evaluating LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# Create an evaluator\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"correctness\", llm=llm)\n",
    "\n",
    "# Define a prediction and reference answer\n",
    "prediction = \"The capital of France is Paris.\"\n",
    "reference = \"Paris is the capital city of France.\"\n",
    "\n",
    "# Evaluate the prediction\n",
    "evaluation_result = evaluator.evaluate_strings(\n",
    "    prediction=prediction,\n",
    "    reference=reference\n",
    ")\n",
    "\n",
    "print(\"Evaluation Result:\", evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099cad8",
   "metadata": {},
   "source": [
    "## 7. Project Implementation\n",
    "\n",
    "Let's put everything together to build a complete project: a document Q&A chatbot with memory and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5493134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "class DocumentQASystem:\n",
    "    def __init__(self, document_path, model_name=\"gpt-3.5-turbo\"):\n",
    "        # Initialize the language model\n",
    "        callback_manager = BaseCallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=model_name,\n",
    "            temperature=0,\n",
    "            streaming=True,\n",
    "            callback_manager=callback_manager\n",
    "        )\n",
    "        \n",
    "        # Load and process documents\n",
    "        self.documents = self._load_documents(document_path)\n",
    "        self.vectorstore = self._create_vectorstore(self.documents)\n",
    "        \n",
    "        # Set up memory and chain\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=self.llm,\n",
    "            retriever=self.vectorstore.as_retriever(),\n",
    "            memory=self.memory,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "    \n",
    "    def _load_documents(self, document_path):\n",
    "        # Load the document\n",
    "        loader = TextLoader(document_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Split the text into chunks\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        return text_splitter.split_documents(documents)\n",
    "    \n",
    "    def _create_vectorstore(self, documents):\n",
    "        # Create embeddings and store in vector database\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        return Chroma.from_documents(documents, embeddings)\n",
    "    \n",
    "    def ask(self, question):\n",
    "        # Get response from QA chain\n",
    "        result = self.qa_chain({\"question\": question})\n",
    "        \n",
    "        # Extract answer and sources\n",
    "        answer = result[\"answer\"]\n",
    "        source_docs = result[\"source_documents\"]\n",
    "        \n",
    "        # Format sources\n",
    "        sources = []\n",
    "        for i, doc in enumerate(source_docs):\n",
    "            source = f\"Source {i+1}: \" + doc.page_content[:100] + \"...\"\n",
    "            sources.append(source)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources\n",
    "        }\n",
    "\n",
    "# Create a sample document\n",
    "with open(\"knowledge_base.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "It enables applications that are context-aware and reason-driven.\n",
    "\n",
    "LangChain consists of several core modules:\n",
    "1. Models: Interfaces to language models from various providers\n",
    "2. Prompts: Templates and techniques for effective prompting\n",
    "3. Memory: State persistence between chain or agent calls\n",
    "4. Indexes: Techniques to structure documents for efficient retrieval\n",
    "5. Chains: Sequences of operations for common tasks\n",
    "6. Agents: LLMs that can use tools based on user inputs\n",
    "\n",
    "One of the key advantages of LangChain is its flexibility. Developers can use the entire framework or just the needed components.\n",
    "\n",
    "LangChain applications can connect to various data sources including APIs, databases, and file systems.\n",
    "\n",
    "The framework supports both Python and JavaScript/TypeScript languages.\n",
    "\n",
    "LangChain was created by Harrison Chase and has gained significant popularity in the AI developer community.\n",
    "\n",
    "Recent updates to LangChain have improved its documentation, added more integrations, and enhanced its agent capabilities.\n",
    "\n",
    "To get started with LangChain, users typically install the package, set up their API keys, and begin with simple chains before moving to more complex applications.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "# Initialize our QA system\n",
    "qa_system = DocumentQASystem(\"knowledge_base.txt\")\n",
    "\n",
    "# Test with some questions\n",
    "print(\"\\nQuestion 1: What is LangChain?\")\n",
    "response = qa_system.ask(\"What is LangChain?\")\n",
    "print(\"\\nSources:\")\n",
    "for source in response[\"sources\"]:\n",
    "    print(f\"- {source}\")\n",
    "\n",
    "print(\"\\nQuestion 2: What are the core modules of LangChain?\")\n",
    "response = qa_system.ask(\"What are the core modules of LangChain?\")\n",
    "print(\"\\nSources:\")\n",
    "for source in response[\"sources\"]:\n",
    "    print(f\"- {source}\")\n",
    "\n",
    "print(\"\\nQuestion 3: Who created LangChain?\")\n",
    "response = qa_system.ask(\"Who created LangChain?\")\n",
    "print(\"\\nSources:\")\n",
    "for source in response[\"sources\"]:\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c72879",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this comprehensive course, we've covered the fundamentals of LangChain and how to use it to build powerful LLM-powered applications. We started with the basics of LangChain components and gradually moved to more complex applications.\n",
    "\n",
    "Here's what we've learned:\n",
    "\n",
    "1. **LangChain Components**:\n",
    "   - Models (LLMs, Chat models)\n",
    "   - Prompts and prompt engineering\n",
    "   - Memory for maintaining context\n",
    "   - Chains for combining operations\n",
    "   - Agents and tools for complex reasoning\n",
    "\n",
    "2. **Building Applications**:\n",
    "   - Question-answering systems\n",
    "   - Chatbots with memory\n",
    "   - Document analysis and summarization\n",
    "   - Multi-tool agents\n",
    "\n",
    "3. **Best Practices**:\n",
    "   - Effective prompt engineering\n",
    "   - Error handling and rate limiting\n",
    "   - Evaluation of LLM outputs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To continue your LangChain journey:\n",
    "\n",
    "1. Explore the [official LangChain documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "2. Join the LangChain Discord community\n",
    "3. Experiment with different models (Anthropic, Hugging Face, etc.)\n",
    "4. Build more complex applications with custom agents and tools\n",
    "\n",
    "Remember that LangChain is rapidly evolving, so keep an eye on updates and new features!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
