{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c619c72c",
   "metadata": {},
   "source": [
    "# GitHub Actions: Practical Training\n",
    "\n",
    "This notebook provides a practical guide to GitHub Actions, focusing on two key workflows:\n",
    "1. Building and pushing Docker images to registries\n",
    "2. Setting up environments for running AI models and plotting their outputs\n",
    "\n",
    "GitHub Actions is a powerful CI/CD platform that allows you to automate various workflows directly from your GitHub repository.\n",
    "\n",
    "## What are GitHub Actions?\n",
    "\n",
    "GitHub Actions is a CI/CD (Continuous Integration and Continuous Deployment) platform that allows you to automate your build, test, and deployment pipeline. It provides:\n",
    "\n",
    "1. **Workflows**: YAML files that define automation processes\n",
    "2. **Events**: Triggers that start workflows (push, pull request, schedule, etc.)\n",
    "3. **Jobs**: Sets of steps that execute on the same runner\n",
    "4. **Steps**: Individual tasks that run commands or actions\n",
    "5. **Actions**: Reusable units of code that can be shared\n",
    "6. **Runners**: Servers that run your workflows (GitHub-hosted or self-hosted)\n",
    "\n",
    "## Basic GitHub Actions Workflow Structure\n",
    "\n",
    "```yaml\n",
    "name: My Workflow\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Run a one-line script\n",
    "        run: echo Hello, world!\n",
    "```\n",
    "\n",
    "Now, let's dive into our specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72c206",
   "metadata": {},
   "source": [
    "## Use Case 1: Building and Pushing Docker Images\n",
    "\n",
    "This workflow demonstrates how to:\n",
    "1. Build a Docker image from your repository\n",
    "2. Test the built image\n",
    "3. Push the image to a Docker registry (Docker Hub or GitHub Container Registry)\n",
    "\n",
    "### Step 1: Create a Dockerfile\n",
    "\n",
    "First, you need a Dockerfile in your repository. Here's a simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20170ec8",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "# Example Dockerfile for a Python application\n",
    "\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "# Set a default command\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```\n",
    "\n",
    "### Step 2: Create GitHub Actions Workflow for Docker\n",
    "\n",
    "Create a file at `.github/workflows/docker-build.yml` with the following content:\n",
    "\n",
    "```yaml\n",
    "name: Build and Push Docker Image\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "    # Optionally trigger on tags to create versioned releases\n",
    "    tags: [ 'v*' ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Docker Buildx\n",
    "        uses: docker/setup-buildx-action@v2\n",
    "      \n",
    "      # Log in to Docker Hub\n",
    "      - name: Login to Docker Hub\n",
    "        if: github.event_name != 'pull_request'\n",
    "        uses: docker/login-action@v2\n",
    "        with:\n",
    "          username: ${{ secrets.DOCKER_USERNAME }}\n",
    "          password: ${{ secrets.DOCKER_PASSWORD }}\n",
    "      \n",
    "      # Extract metadata for the Docker image\n",
    "      - name: Extract Docker metadata\n",
    "        id: meta\n",
    "        uses: docker/metadata-action@v4\n",
    "        with:\n",
    "          images: username/my-application\n",
    "          # Generate tags based on the following events\n",
    "          tags: |\n",
    "            type=ref,event=branch\n",
    "            type=ref,event=pr\n",
    "            type=semver,pattern={{version}}\n",
    "            type=semver,pattern={{major}}.{{minor}}\n",
    "            type=sha\n",
    "      \n",
    "      # Build and push the Docker image\n",
    "      - name: Build and push Docker image\n",
    "        uses: docker/build-push-action@v4\n",
    "        with:\n",
    "          context: .\n",
    "          push: ${{ github.event_name != 'pull_request' }}\n",
    "          tags: ${{ steps.meta.outputs.tags }}\n",
    "          labels: ${{ steps.meta.outputs.labels }}\n",
    "          cache-from: type=gha\n",
    "          cache-to: type=gha,mode=max\n",
    "```\n",
    "\n",
    "### Setting Up the Required Secrets\n",
    "\n",
    "For the workflow above, you need to add secrets to your repository:\n",
    "\n",
    "1. Go to your repository on GitHub\n",
    "2. Navigate to \"Settings\" > \"Secrets and variables\" > \"Actions\"\n",
    "3. Add the following secrets:\n",
    "   - `DOCKER_USERNAME`: Your Docker Hub username\n",
    "   - `DOCKER_PASSWORD`: Your Docker Hub access token (not your account password)\n",
    "\n",
    "### Alternative: Using GitHub Container Registry\n",
    "\n",
    "If you prefer to use GitHub Container Registry (ghcr.io) instead of Docker Hub:\n",
    "\n",
    "```yaml\n",
    "# Replace the Docker Hub login step with:\n",
    "- name: Login to GitHub Container Registry\n",
    "  if: github.event_name != 'pull_request'\n",
    "  uses: docker/login-action@v2\n",
    "  with:\n",
    "    registry: ghcr.io\n",
    "    username: ${{ github.repository_owner }}\n",
    "    password: ${{ secrets.GITHUB_TOKEN }}\n",
    "\n",
    "# And update the images in metadata:\n",
    "images: ghcr.io/${{ github.repository }}\n",
    "```\n",
    "\n",
    "No additional secrets are required for GitHub Container Registry as `GITHUB_TOKEN` is automatically provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70355f22",
   "metadata": {},
   "source": [
    "## Use Case 2: Environment for Running AI Models and Plotting Outputs\n",
    "\n",
    "This workflow demonstrates how to:\n",
    "1. Set up a Python environment with ML libraries\n",
    "2. Run AI model training or inference\n",
    "3. Generate and save plots/visualizations\n",
    "4. Upload results as artifacts\n",
    "\n",
    "### Step 1: Create GitHub Actions Workflow for AI Model Training\n",
    "\n",
    "Create a file at `.github/workflows/ai-model-training.yml` with the following content:\n",
    "\n",
    "```yaml\n",
    "name: AI Model Training and Visualization\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "    paths:\n",
    "      - 'model/**'\n",
    "      - 'data/**'\n",
    "  workflow_dispatch:  # Allows manual triggering\n",
    "    inputs:\n",
    "      epochs:\n",
    "        description: 'Number of training epochs'\n",
    "        required: true\n",
    "        default: '10'\n",
    "      batch_size:\n",
    "        description: 'Batch size for training'\n",
    "        required: true\n",
    "        default: '32'\n",
    "\n",
    "jobs:\n",
    "  train:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "          cache: 'pip'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "          # Install additional visualization libraries\n",
    "          pip install matplotlib seaborn plotly\n",
    "      \n",
    "      - name: Download dataset (if needed)\n",
    "        run: |\n",
    "          mkdir -p data\n",
    "          # Example: Download a dataset from a public source\n",
    "          # wget -O data/dataset.csv https://example.com/dataset.csv\n",
    "          echo \"Using existing dataset or downloading as needed\"\n",
    "      \n",
    "      - name: Train model\n",
    "        run: |\n",
    "          # Set epochs from workflow input or use default\n",
    "          EPOCHS=${{ github.event.inputs.epochs || '10' }}\n",
    "          BATCH_SIZE=${{ github.event.inputs.batch_size || '32' }}\n",
    "          \n",
    "          # Run training script with parameters\n",
    "          python model/train.py --epochs $EPOCHS --batch_size $BATCH_SIZE --output_dir ./output\n",
    "      \n",
    "      - name: Generate visualizations\n",
    "        run: |\n",
    "          # Run script to generate plots\n",
    "          python model/visualize.py --model_dir ./output --plots_dir ./plots\n",
    "      \n",
    "      - name: Archive model artifacts\n",
    "        uses: actions/upload-artifact@v3\n",
    "        with:\n",
    "          name: model-artifacts\n",
    "          path: |\n",
    "            output/*.h5\n",
    "            output/*.pkl\n",
    "            output/metrics.json\n",
    "      \n",
    "      - name: Archive plots\n",
    "        uses: actions/upload-artifact@v3\n",
    "        with:\n",
    "          name: training-plots\n",
    "          path: plots/\n",
    "```\n",
    "\n",
    "### Step 2: Example Python Scripts\n",
    "\n",
    "To complement the GitHub Actions workflow, here are example Python scripts for training and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdfb201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: model/train.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def main():\n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Train a simple neural network')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output', help='Directory to save model and outputs')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load and preprocess data (example with MNIST)\n",
    "    print(\"Loading dataset...\")\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "    \n",
    "    # Build a simple model\n",
    "    print(\"Building model...\")\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(args.output_dir, 'model_checkpoint.h5'),\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy'\n",
    "        ),\n",
    "        tf.keras.callbacks.CSVLogger(os.path.join(args.output_dir, 'training_log.csv'))\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"Training model for {args.epochs} epochs with batch size {args.batch_size}...\")\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "        validation_split=0.1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model...\")\n",
    "    test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(f\"Test loss: {test_scores[0]}\")\n",
    "    print(f\"Test accuracy: {test_scores[1]}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    model.save(os.path.join(args.output_dir, 'final_model.h5'))\n",
    "    print(f\"Saved model to {args.output_dir}/final_model.h5\")\n",
    "    \n",
    "    # Save training history and test metrics\n",
    "    metrics = {\n",
    "        'test_loss': float(test_scores[0]),\n",
    "        'test_accuracy': float(test_scores[1]),\n",
    "        'training_history': {\n",
    "            'accuracy': [float(x) for x in history.history['accuracy']],\n",
    "            'val_accuracy': [float(x) for x in history.history['val_accuracy']],\n",
    "            'loss': [float(x) for x in history.history['loss']],\n",
    "            'val_loss': [float(x) for x in history.history['val_loss']]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(args.output_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e938ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: model/visualize.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def plot_training_history(history_dict, output_dir):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_dict['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history_dict['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_dict['loss'], label='Training Loss')\n",
    "    plt.plot(history_dict['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'training_history.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(model, output_dir):\n",
    "    \"\"\"Generate and plot confusion matrix.\"\"\"\n",
    "    # Load test data\n",
    "    (_, _), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = tf.math.confusion_matrix(y_test, y_pred).numpy()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_sample_predictions(model, output_dir):\n",
    "    \"\"\"Plot sample images and their predictions.\"\"\"\n",
    "    # Load test data\n",
    "    (_, _), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_test_norm = x_test.astype(\"float32\") / 255.0\n",
    "    \n",
    "    # Get predictions for a few samples\n",
    "    predictions = model.predict(x_test_norm[:20])\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(20):\n",
    "        plt.subplot(4, 5, i+1)\n",
    "        plt.imshow(x_test[i], cmap='gray')\n",
    "        color = 'green' if predicted_classes[i] == y_test[i] else 'red'\n",
    "        plt.title(f\"True: {y_test[i]}, Pred: {predicted_classes[i]}\", \n",
    "                  color=color)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'sample_predictions.png'))\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Generate visualizations for trained model')\n",
    "    parser.add_argument('--model_dir', type=str, default='./output', \n",
    "                        help='Directory where model and metrics are saved')\n",
    "    parser.add_argument('--plots_dir', type=str, default='./plots', \n",
    "                        help='Directory to save plots')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(args.plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Load metrics\n",
    "    print(\"Loading metrics...\")\n",
    "    try:\n",
    "        with open(os.path.join(args.model_dir, 'metrics.json'), 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Metrics file not found!\")\n",
    "        return\n",
    "    \n",
    "    # Plot training history\n",
    "    print(\"Generating training history plot...\")\n",
    "    plot_training_history(metrics['training_history'], args.plots_dir)\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    try:\n",
    "        model_path = os.path.join(args.model_dir, 'final_model.h5')\n",
    "        model = keras.models.load_model(model_path)\n",
    "    except:\n",
    "        print(\"Error: Could not load model!\")\n",
    "        return\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    print(\"Generating confusion matrix...\")\n",
    "    plot_confusion_matrix(model, args.plots_dir)\n",
    "    \n",
    "    # Plot sample predictions\n",
    "    print(\"Generating sample prediction plots...\")\n",
    "    plot_sample_predictions(model, args.plots_dir)\n",
    "    \n",
    "    print(f\"All visualizations saved to {args.plots_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e9b88",
   "metadata": {},
   "source": [
    "### Step 3: Setting Up Requirements.txt\n",
    "\n",
    "Create a `requirements.txt` file in the root of your repository:\n",
    "\n",
    "```\n",
    "numpy>=1.20.0\n",
    "tensorflow>=2.8.0\n",
    "matplotlib>=3.5.0\n",
    "seaborn>=0.11.0\n",
    "scikit-learn>=1.0.0\n",
    "```\n",
    "\n",
    "## Combining Both Workflows: AI Model in Docker\n",
    "\n",
    "Now, let's combine our two use cases by creating a workflow that:\n",
    "1. Trains an AI model\n",
    "2. Creates visualizations\n",
    "3. Packages everything into a Docker image\n",
    "4. Pushes the image to a registry\n",
    "\n",
    "### Create a Dockerfile for the AI Model\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy model code and trained artifacts\n",
    "COPY model/ ./model/\n",
    "COPY output/ ./output/\n",
    "COPY plots/ ./plots/\n",
    "\n",
    "# Set up a command to serve predictions or show results\n",
    "CMD [\"python\", \"model/serve.py\", \"--model_path\", \"output/final_model.h5\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "### Create a Combined Workflow\n",
    "\n",
    "Create a file at `.github/workflows/train-and-dockerize.yml`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435abc6f",
   "metadata": {},
   "source": [
    "```yaml\n",
    "name: Train AI Model and Dockerize\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "    paths:\n",
    "      - 'model/**'\n",
    "      - 'data/**'\n",
    "  workflow_dispatch:\n",
    "    inputs:\n",
    "      epochs:\n",
    "        description: 'Number of training epochs'\n",
    "        required: true\n",
    "        default: '10'\n",
    "\n",
    "jobs:\n",
    "  train:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "          cache: 'pip'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "      \n",
    "      - name: Train model\n",
    "        run: |\n",
    "          EPOCHS=${{ github.event.inputs.epochs || '10' }}\n",
    "          python model/train.py --epochs $EPOCHS --output_dir ./output\n",
    "      \n",
    "      - name: Generate visualizations\n",
    "        run: |\n",
    "          python model/visualize.py --model_dir ./output --plots_dir ./plots\n",
    "      \n",
    "      - name: Upload artifacts for next job\n",
    "        uses: actions/upload-artifact@v3\n",
    "        with:\n",
    "          name: model-artifacts\n",
    "          path: |\n",
    "            model/\n",
    "            output/\n",
    "            plots/\n",
    "            requirements.txt\n",
    "  \n",
    "  dockerize:\n",
    "    needs: train\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Download artifacts\n",
    "        uses: actions/download-artifact@v3\n",
    "        with:\n",
    "          name: model-artifacts\n",
    "          path: .\n",
    "      \n",
    "      - name: Set up Docker Buildx\n",
    "        uses: docker/setup-buildx-action@v2\n",
    "      \n",
    "      - name: Login to Docker Hub\n",
    "        uses: docker/login-action@v2\n",
    "        with:\n",
    "          username: ${{ secrets.DOCKER_USERNAME }}\n",
    "          password: ${{ secrets.DOCKER_PASSWORD }}\n",
    "      \n",
    "      - name: Extract Docker metadata\n",
    "        id: meta\n",
    "        uses: docker/metadata-action@v4\n",
    "        with:\n",
    "          images: username/ai-model-app\n",
    "          tags: |\n",
    "            type=sha,format=short\n",
    "            type=raw,value=latest,enable=${{ github.ref == format('refs/heads/{0}', github.event.repository.default_branch) }}\n",
    "      \n",
    "      - name: Build and push Docker image\n",
    "        uses: docker/build-push-action@v4\n",
    "        with:\n",
    "          context: .\n",
    "          push: true\n",
    "          tags: ${{ steps.meta.outputs.tags }}\n",
    "          labels: ${{ steps.meta.outputs.labels }}\n",
    "```\n",
    "\n",
    "This workflow first trains the model and generates visualizations, then builds and pushes a Docker image containing the trained model and visualization outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f346e0b",
   "metadata": {},
   "source": [
    "## Advanced GitHub Actions Features\n",
    "\n",
    "### 1. Matrix Builds for Testing Different Configurations\n",
    "\n",
    "You can use matrix builds to test your model with different configurations:\n",
    "\n",
    "```yaml\n",
    "jobs:\n",
    "  train:\n",
    "    runs-on: ubuntu-latest\n",
    "    strategy:\n",
    "      matrix:\n",
    "        learning_rate: [0.001, 0.01]\n",
    "        batch_size: [32, 64]\n",
    "    \n",
    "    steps:\n",
    "      # ... other steps\n",
    "      - name: Train model\n",
    "        run: |\n",
    "          python model/train.py \\\n",
    "            --learning_rate ${{ matrix.learning_rate }} \\\n",
    "            --batch_size ${{ matrix.batch_size }} \\\n",
    "            --output_dir ./output/${{ matrix.learning_rate }}_${{ matrix.batch_size }}\n",
    "```\n",
    "\n",
    "### 2. Caching Dependencies and Data\n",
    "\n",
    "Improve workflow performance by caching dependencies and datasets:\n",
    "\n",
    "```yaml\n",
    "- name: Cache pip dependencies\n",
    "  uses: actions/cache@v3\n",
    "  with:\n",
    "    path: ~/.cache/pip\n",
    "    key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}\n",
    "    restore-keys: |\n",
    "      ${{ runner.os }}-pip-\n",
    "\n",
    "- name: Cache dataset\n",
    "  uses: actions/cache@v3\n",
    "  with:\n",
    "    path: ./data\n",
    "    key: dataset-v1  # Increment this version when your dataset changes\n",
    "```\n",
    "\n",
    "### 3. Scheduled Training Runs\n",
    "\n",
    "Run periodic training on new data with a schedule:\n",
    "\n",
    "```yaml\n",
    "on:\n",
    "  schedule:\n",
    "    # Run every Monday at 3:00 AM\n",
    "    - cron: '0 3 * * 1'\n",
    "```\n",
    "\n",
    "### 4. Environment-specific Deployment\n",
    "\n",
    "Deploy trained models to different environments based on branch:\n",
    "\n",
    "```yaml\n",
    "jobs:\n",
    "  deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    environment: ${{ github.ref == 'refs/heads/main' && 'production' || 'staging' }}\n",
    "    steps:\n",
    "      - name: Deploy model\n",
    "        run: |\n",
    "          python deploy.py \\\n",
    "            --model ./output/final_model.h5 \\\n",
    "            --environment ${{ env.ENVIRONMENT_NAME }} \\\n",
    "            --api_key ${{ secrets.API_KEY }}\n",
    "```\n",
    "\n",
    "## GitHub Actions Security Best Practices\n",
    "\n",
    "1. **Use Secrets for Sensitive Data**: Always use GitHub Secrets for API keys, tokens, and credentials.\n",
    "\n",
    "2. **Limit Permissions**: Use the `permissions` keyword to limit what the workflow can do.\n",
    "\n",
    "3. **Pin Action Versions**: Always pin actions to specific versions or commit SHAs.\n",
    "\n",
    "4. **Validate External Inputs**: Be cautious with inputs from external PRs and validate them.\n",
    "\n",
    "5. **Secure Docker Images**: Scan Docker images for vulnerabilities using:\n",
    "\n",
    "```yaml\n",
    "- name: Scan Docker image\n",
    "  uses: aquasecurity/trivy-action@master\n",
    "  with:\n",
    "    image-ref: 'username/ai-model-app:latest'\n",
    "    format: 'table'\n",
    "    exit-code: '1'\n",
    "    severity: 'CRITICAL'\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "GitHub Actions provides a powerful platform for automating your AI model training and Docker image building workflows. By leveraging these tools, you can:\n",
    "\n",
    "1. Automatically train and evaluate models with each code change\n",
    "2. Generate and archive visualizations\n",
    "3. Package models into Docker containers for deployment\n",
    "4. Ensure consistent environment across development and production\n",
    "\n",
    "For more complex workflows, you can explore additional features like:\n",
    "- Self-hosted runners for specialized hardware (e.g., GPU instances)\n",
    "- Integration with cloud services (AWS, Azure, GCP)\n",
    "- Notifications via email, Slack, or other platforms\n",
    "- Model deployment to production environments\n",
    "\n",
    "This notebook provides a foundation for implementing these workflows in your own repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db568da",
   "metadata": {},
   "source": [
    "## Exercise: Create Your Own GitHub Actions Workflow\n",
    "\n",
    "Try creating a simple GitHub Actions workflow that:\n",
    "\n",
    "1. Sets up a Python environment\n",
    "2. Installs dependencies from a requirements.txt file\n",
    "3. Runs a simple script to train a model (you can use the provided examples)\n",
    "4. Saves the results as artifacts\n",
    "\n",
    "Steps:\n",
    "1. Create a `.github/workflows/exercise.yml` file\n",
    "2. Define a workflow that triggers on push to the main branch\n",
    "3. Set up a job that runs on ubuntu-latest\n",
    "4. Add steps to checkout code, set up Python, and install dependencies\n",
    "5. Add a step to run a training script\n",
    "6. Add a step to save artifacts\n",
    "\n",
    "Good luck! Once you push this workflow to GitHub, it will automatically run whenever you push changes to the main branch."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
