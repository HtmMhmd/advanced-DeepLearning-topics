{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc615d53",
   "metadata": {},
   "source": [
    "# ChromaDB: A Simple Course\n",
    "\n",
    "ChromaDB is an open-source embedding database designed for storing and retrieving vector embeddings efficiently. This notebook provides a hands-on introduction to using ChromaDB in Python.\n",
    "\n",
    "## What is ChromaDB?\n",
    "\n",
    "ChromaDB is a database built specifically for AI applications that need to store and query vector embeddings. It's particularly useful for:\n",
    "\n",
    "- Semantic search\n",
    "- Recommendation systems\n",
    "- Retrieval-augmented generation (RAG)\n",
    "- Similar document detection\n",
    "- Image search (when using image embeddings)\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Simple API**: Easy to use from Python\n",
    "- **Persistent storage**: Save your embeddings to disk\n",
    "- **Fast similarity search**: Efficient nearest neighbor algorithms\n",
    "- **Multi-modal**: Store embeddings of text, images, or other data\n",
    "- **Metadata filtering**: Query based on both vector similarity and metadata\n",
    "- **Multiple collections**: Organize different types of embeddings\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ChromaDB\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df796336",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "Let's start with the fundamentals of ChromaDB:\n",
    "\n",
    "1. Creating a client\n",
    "2. Creating a collection\n",
    "3. Adding documents with embeddings\n",
    "4. Querying similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "\n",
    "# Create an in-memory client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create a collection\n",
    "collection = client.create_collection(name=\"documents\")\n",
    "\n",
    "# Add documents with embeddings\n",
    "collection.add(\n",
    "    ids=[\"doc1\", \"doc2\", \"doc3\"],\n",
    "    documents=[\n",
    "        \"ChromaDB is a database for storing embeddings\",\n",
    "        \"Embeddings are vector representations of data\",\n",
    "        \"Vector databases make semantic search possible\"\n",
    "    ],\n",
    "    # We're providing our own embeddings here as an example\n",
    "    # (normally you might use an embedding function)\n",
    "    embeddings=[\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],  # Embedding for doc1\n",
    "        [0.5, 0.6, 0.7, 0.8, 0.9],  # Embedding for doc2\n",
    "        [0.1, 0.3, 0.5, 0.7, 0.9]   # Embedding for doc3\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Query similar documents with a query embedding\n",
    "results = collection.query(\n",
    "    query_embeddings=[[0.1, 0.2, 0.3, 0.4, 0.5]],  # Similar to doc1\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print(\"Query results:\")\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    print(f\"{i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41d8ea",
   "metadata": {},
   "source": [
    "## Using Embedding Functions\n",
    "\n",
    "Rather than manually creating embeddings, ChromaDB can integrate with popular embedding models via embedding functions. Let's see how to use a Sentence Transformer embedding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies if not already installed\n",
    "!pip install -q sentence-transformers\n",
    "\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Create an embedding function using Sentence Transformers\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"  # A good, small model for embeddings\n",
    ")\n",
    "\n",
    "# Create a new collection with the embedding function\n",
    "collection_with_embeddings = client.create_collection(\n",
    "    name=\"documents_with_embedding_function\", \n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# Now we can add documents without providing embeddings\n",
    "collection_with_embeddings.add(\n",
    "    ids=[\"doc1\", \"doc2\", \"doc3\", \"doc4\"],\n",
    "    documents=[\n",
    "        \"ChromaDB stores vector embeddings efficiently\",\n",
    "        \"Neural networks can create embedding vectors from text\",\n",
    "        \"Semantic search finds results based on meaning, not just keywords\",\n",
    "        \"Vector similarity is often computed using cosine distance\"\n",
    "    ]\n",
    "    # No need to provide embeddings - they're generated automatically\n",
    ")\n",
    "\n",
    "# Query with text instead of embeddings\n",
    "results = collection_with_embeddings.query(\n",
    "    query_texts=[\"How do embeddings work with neural networks?\"],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print(\"Query results:\")\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    print(f\"{i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e016e",
   "metadata": {},
   "source": [
    "## Persistent Storage\n",
    "\n",
    "In real applications, you'll want to save your embeddings to disk rather than keeping them in memory. Here's how to use persistent storage with ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4273bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a temporary directory for our persistent database\n",
    "persist_directory = tempfile.mkdtemp()\n",
    "print(f\"Using persistent directory: {persist_directory}\")\n",
    "\n",
    "# Create a persistent client\n",
    "persistent_client = chromadb.PersistentClient(path=persist_directory)\n",
    "\n",
    "# Create a collection\n",
    "persistent_collection = persistent_client.create_collection(\n",
    "    name=\"persistent_docs\",\n",
    "    embedding_function=embedding_function  # Reusing the function from before\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "persistent_collection.add(\n",
    "    ids=[\"doc1\", \"doc2\", \"doc3\"],\n",
    "    documents=[\n",
    "        \"This document will be saved to disk\",\n",
    "        \"The embeddings are stored persistently\",\n",
    "        \"You can restart your application and still access the collection\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Query\n",
    "results = persistent_collection.query(\n",
    "    query_texts=[\"persistent storage\"],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print(\"Query results:\")\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    print(f\"{i+1}. {doc}\")\n",
    "\n",
    "# If you restart your application, you can load the collection again:\n",
    "# reloaded_client = chromadb.PersistentClient(path=persist_directory)\n",
    "# reloaded_collection = reloaded_client.get_collection(\"persistent_docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdabdf",
   "metadata": {},
   "source": [
    "## Working with Metadata\n",
    "\n",
    "ChromaDB allows you to store metadata alongside your documents and embeddings. This metadata can be used for filtering during queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3544c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection for documents with metadata\n",
    "collection_with_metadata = client.create_collection(\n",
    "    name=\"documents_with_metadata\", \n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# Add documents with metadata\n",
    "collection_with_metadata.add(\n",
    "    ids=[\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\"],\n",
    "    documents=[\n",
    "        \"Python is a popular programming language\",\n",
    "        \"JavaScript is commonly used for web development\",\n",
    "        \"TensorFlow is a machine learning framework\",\n",
    "        \"PyTorch is another machine learning framework\",\n",
    "        \"SQL is used for database queries\"\n",
    "    ],\n",
    "    metadatas=[\n",
    "        {\"type\": \"language\", \"level\": \"beginner\", \"year\": 2023},\n",
    "        {\"type\": \"language\", \"level\": \"intermediate\", \"year\": 2023},\n",
    "        {\"type\": \"framework\", \"level\": \"advanced\", \"year\": 2022},\n",
    "        {\"type\": \"framework\", \"level\": \"advanced\", \"year\": 2022},\n",
    "        {\"type\": \"language\", \"level\": \"intermediate\", \"year\": 2021}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Query with metadata filtering\n",
    "results = collection_with_metadata.query(\n",
    "    query_texts=[\"machine learning libraries\"],\n",
    "    where={\"type\": \"framework\"},  # Only return frameworks\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(\"Query results (frameworks only):\")\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    print(f\"{i+1}. {doc}\")\n",
    "\n",
    "# Query with another metadata filter\n",
    "results = collection_with_metadata.query(\n",
    "    query_texts=[\"programming languages\"],\n",
    "    where={\"level\": \"intermediate\"},  # Only intermediate level\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(\"\\nQuery results (intermediate level only):\")\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    print(f\"{i+1}. {doc}\")\n",
    "\n",
    "# Query with a more complex metadata filter\n",
    "results = collection_with_metadata.query(\n",
    "    query_texts=[\"programming technologies\"],\n",
    "    where={\"year\": {\"$gte\": 2022}},  # Only from 2022 or later\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(\"\\nQuery results (from 2022 or later):\")\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    print(f\"{i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad8dde",
   "metadata": {},
   "source": [
    "## Advanced Usage: Updating and Deleting\n",
    "\n",
    "ChromaDB supports updating and deleting documents from collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1aff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection\n",
    "update_collection = client.create_collection(\n",
    "    name=\"documents_to_update\", \n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# Add initial documents\n",
    "update_collection.add(\n",
    "    ids=[\"doc1\", \"doc2\", \"doc3\"],\n",
    "    documents=[\n",
    "        \"Initial version of document 1\",\n",
    "        \"Initial version of document 2\",\n",
    "        \"This document will be deleted\"\n",
    "    ],\n",
    "    metadatas=[\n",
    "        {\"version\": 1},\n",
    "        {\"version\": 1},\n",
    "        {\"version\": 1}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Update documents\n",
    "update_collection.update(\n",
    "    ids=[\"doc1\", \"doc2\"],\n",
    "    documents=[\n",
    "        \"Updated version of document 1\",\n",
    "        \"Updated version of document 2\"\n",
    "    ],\n",
    "    metadatas=[\n",
    "        {\"version\": 2},\n",
    "        {\"version\": 2}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Delete a document\n",
    "update_collection.delete(\n",
    "    ids=[\"doc3\"]\n",
    ")\n",
    "\n",
    "# Get all documents\n",
    "results = update_collection.get()\n",
    "\n",
    "print(\"Documents after updates and deletion:\")\n",
    "for i, (doc_id, doc, metadata) in enumerate(zip(results['ids'], results['documents'], results['metadatas'])):\n",
    "    print(f\"{i+1}. ID: {doc_id}, Content: '{doc}', Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f1a00",
   "metadata": {},
   "source": [
    "## Integration with RAG Systems\n",
    "\n",
    "ChromaDB is particularly well-suited for Retrieval-Augmented Generation (RAG) systems. Let's see how it fits into a simple RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa99f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple RAG pipeline with ChromaDB\n",
    "\n",
    "# 1. Set up collection\n",
    "rag_collection = client.create_collection(\n",
    "    name=\"rag_documents\", \n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# 2. Add knowledge base documents\n",
    "rag_collection.add(\n",
    "    ids=[\"kb1\", \"kb2\", \"kb3\", \"kb4\"],\n",
    "    documents=[\n",
    "        \"The capital of France is Paris. It is known for the Eiffel Tower.\",\n",
    "        \"Rome is the capital of Italy. The Colosseum is located in Rome.\",\n",
    "        \"Japan is an island country in East Asia. Tokyo is its capital.\",\n",
    "        \"The Great Wall of China is over 13,000 miles long.\"\n",
    "    ],\n",
    "    metadatas=[\n",
    "        {\"source\": \"geography\", \"topic\": \"France\"},\n",
    "        {\"source\": \"geography\", \"topic\": \"Italy\"},\n",
    "        {\"source\": \"geography\", \"topic\": \"Japan\"},\n",
    "        {\"source\": \"geography\", \"topic\": \"China\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. Function to simulate LLM response (in a real system, you would call an LLM API)\n",
    "def simulate_llm_response(question, context):\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Context provided to LLM:\")\n",
    "    for i, ctx in enumerate(context):\n",
    "        print(f\"  {i+1}. {ctx}\")\n",
    "    \n",
    "    # In a real system, you would call an API like OpenAI here\n",
    "    # For this example, we'll just return a simple response\n",
    "    if \"France\" in \" \".join(context):\n",
    "        return \"The capital of France is Paris, which is famous for the Eiffel Tower.\"\n",
    "    elif \"Italy\" in \" \".join(context):\n",
    "        return \"Rome is the capital city of Italy. It's known for the ancient Colosseum.\"\n",
    "    elif \"Japan\" in \" \".join(context):\n",
    "        return \"Tokyo is the capital city of Japan, which is an island nation in East Asia.\"\n",
    "    elif \"China\" in \" \".join(context):\n",
    "        return \"The Great Wall of China is one of the most impressive structures ever built, stretching over 13,000 miles.\"\n",
    "    else:\n",
    "        return \"I don't have enough context to answer that question.\"\n",
    "\n",
    "# 4. RAG function\n",
    "def answer_with_rag(question):\n",
    "    # Retrieve relevant context\n",
    "    results = rag_collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=2\n",
    "    )\n",
    "    \n",
    "    context = results['documents'][0]\n",
    "    \n",
    "    # Generate answer using the context\n",
    "    answer = simulate_llm_response(question, context)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the RAG system\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Tell me about Rome\",\n",
    "    \"What's interesting about the Great Wall of China?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    answer = answer_with_rag(question)\n",
    "    print(\"\\nGenerated answer:\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12480f0",
   "metadata": {},
   "source": [
    "## Best Practices for ChromaDB\n",
    "\n",
    "Here are some best practices for working with ChromaDB in production:\n",
    "\n",
    "1. **Choose the right embedding model**: The quality of your embeddings significantly impacts retrieval performance.\n",
    "\n",
    "2. **Index size consideration**: Large collections might require more sophisticated retrieval methods.\n",
    "\n",
    "3. **Chunk size matters**: For text documents, how you chunk your text affects retrieval quality.\n",
    "\n",
    "4. **Metadata for filtering**: Use metadata to enable efficient filtering.\n",
    "\n",
    "5. **Regular backups**: With persistent storage, ensure you have backup strategies.\n",
    "\n",
    "6. **Collection organization**: Create separate collections for different types of data.\n",
    "\n",
    "7. **Monitor query performance**: As your collections grow, keep an eye on query latency.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "ChromaDB offers an elegant solution for storing and querying vector embeddings. Its integration with popular embedding models and efficient similarity search makes it ideal for RAG applications, semantic search, and other AI use cases requiring vector similarity.\n",
    "\n",
    "For more information, check out the [official documentation](https://docs.trychroma.com/)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
